<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>RNN 和 CNN 基础</title>
    <link href="/2023/12/01/RNN&amp;CNN/"/>
    <url>/2023/12/01/RNN&amp;CNN/</url>
    
    <content type="html"><![CDATA[<h1 id="rnn-和-cnn-基础">RNN 和 CNN 基础</h1><h2 id="gru">GRU</h2><p>英文：Gated Recurrent Unit</p><p>中文：门控循环单元</p><p>门控循环神经网络包含了两个主要的门：更新门（UpdateGate）和重置门（Reset Gate）。这些门控制着信息在序列中的流动，使得 GRU能够更好地处理长期依赖关系。</p><p> </p><h3 id="reset-gate">Reset Gate</h3><p><strong>公式：</strong><spanclass="math inline">\(r_t=\sigma(W_{ir}x_t+b_{ir}+W_{hr}h_{t-1}+b_{hr})\)</span></p><p><strong>参数说明：</strong></p><ul><li><span class="math inline">\(r_t\)</span> 是重置门在时间步 <spanclass="math inline">\(t\)</span> 的输出，<spanclass="math inline">\(\sigma\)</span> 是 Sigmoid 激活函数</li><li><span class="math inline">\(W_{ir}\)</span> 和 <spanclass="math inline">\(b_{ir}\)</span> 分别是输入门的权重和偏重</li><li><span class="math inline">\(W_{hr}\)</span> 和 <spanclass="math inline">\(b_{hr}\)</span> 分别是隐藏状态的权重和偏重</li></ul><p><strong>作用：</strong>控制当前时间步 <spanclass="math inline">\(t\)</span> 的候选隐藏状态 <spanclass="math inline">\(\widehat{h_t}\)</span>，有多少信息来自前一时间步的隐藏状态<span class="math inline">\(h_{t-1}\)</span>。</p><p>通过重置门，模型可以选择性地遗忘一些先前的信息，从而更灵活地适应不同时间步的输入。“重置”一词表示了它在重新设置或调整计算过程中的权重，以及在某种程度上“重置”隐藏状态的作用。</p><p> </p><h3 id="update-gate">Update Gate</h3><p><strong>公式：</strong><spanclass="math inline">\(z_t=\sigma(W_{iz}x_t+b_{iz}+W_{hz}h_{t-1}+b_{hz})\)</span></p><p><strong>参数说明：</strong></p><ul><li><span class="math inline">\(z_t\)</span> 是更新门在时间步 <spanclass="math inline">\(t\)</span> 的输出，<spanclass="math inline">\(\sigma\)</span> 是 Sigmoid 激活函数</li><li><span class="math inline">\(W_{iz}\)</span> 和 <spanclass="math inline">\(b_{iz}\)</span> 分别是输入门的权重和偏重</li><li><span class="math inline">\(W_{hz}\)</span> 和 <spanclass="math inline">\(b_{hz}\)</span> 分别是隐藏状态的权重和偏重</li></ul><p><strong>作用：</strong><span class="math inline">\(z_t\)</span>取值在 0 和 1 之间，它决定了保留多少先前的信息加入到新的信息。</p><p>即控制当前时间步 <span class="math inline">\(t\)</span> 的隐藏状态<span class="math inline">\(h_t\)</span></p><ul><li>有多少信息来自前一时间步的隐藏状态 <spanclass="math inline">\(h_{t-1}\)</span></li><li>有多少信息来自当前时间步的候选隐藏状态 <spanclass="math inline">\(\widehat{h_t}\)</span></li></ul><p> </p><h3 id="candidate-context">Candidate Context</h3><p><spanclass="math inline">\(\widehat{h_t}=tanh(W_{ih}x_t+b_{ih}+r_{t}\odot(W_{hh}h_{t-1}+b_{hh})))\)</span></p><p>参数说明：</p><ul><li><span class="math inline">\(\widehat{h_t}\)</span> 是在时间步 <spanclass="math inline">\(t\)</span> 的候选隐藏状态</li><li><span class="math inline">\(tanh\)</span> 是双曲正切激活函数</li><li><span class="math inline">\(\odot\)</span> 是逐元素乘法</li></ul><p>最终 <span class="math inline">\(h_t=(1-z_t)\odoth_{t-1}+z_t\odot\widehat{h_t}\)</span></p>]]></content>
    
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Graph Attention Recurrent Neural Networks for Correlated Time Series Forecasting</title>
    <link href="/2023/12/01/GA-RNN/"/>
    <url>/2023/12/01/GA-RNN/</url>
    
    <content type="html"><![CDATA[<h1id="graph-attention-recurrent-neural-networks-for-correlated-time-series-forecasting">GraphAttention Recurrent Neural Networks for Correlated Time SeriesForecasting</h1><p><strong>参考博客</strong></p><p>知乎：<ahref="https://www.zhihu.com/question/288081659/answer/1222002868">Attention跟一维卷积的区别是啥？</a></p><p><strong>核心思想</strong></p><p>RNN 结合 Attention 里有 Attention 结合 CNN</p><p> </p><h2 id="abstract">Abstract</h2><p><strong>GA-RNN模型</strong>主要是为了处理<strong>多个实体随时间相互作用</strong>的情况，其中实体随时间变化的状态（time-varyingstatus）可以被表示为一个时间序列（timeseries），而且这些时间序列是相互关联（correlated）的。</p><p><strong>核心场景：</strong>在公路的不同位置上都部署有速度传感器，这些传感器全天捕捉的车速将形成一个时间序列，并且各个时间序列之间通常是有联系的。</p><blockquote><p>训练集：速度时间序列数据集。</p></blockquote><p>下图是一个例子。在早上 7:00 的时候，实体 D 和 B、C的相关性很强，而在早上 9:00 的时候，实体 D 和 E、F的相关性很强。所以实体之间的相关性是随时间动态变化的。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GARNN1.png"alt="动态相互作用" /><figcaption aria-hidden="true">动态相互作用</figcaption></figure><p><strong>本文的 IDEA：</strong></p><ol type="1"><li>空间方面：时间序列之间是相互关联的，因此采用 attention进行捕捉。</li><li>时间方面：时间序列内部存在时间依赖性，因此采用 GRU 进行捕捉。</li></ol><blockquote><p>先 1 后 2，且 2 结合 1，从而能够捕捉时空关系（spatio-temporalcorrelations）</p></blockquote><p> </p><h2 id="introduction">Introduction</h2><p><strong>GA-RNN 简述：</strong></p><ol type="1"><li>作者根据空间邻近性（spatialproximity）来搭建了一个图。在本文中，实体 entity/entities 被称为vertex/vertices，用邻接矩阵来表示两个实体之间是否有 edge 边。</li><li>为每个实体计算 attentionscore，以考察该实体和它的哪个邻居（1-hop）的相关性更高，事实上该值还被用作了邻接矩阵中的值。</li><li>使用 RNN 来捕捉时空依赖关系（spatio-temporaldependency），作者还将经典 RNN 单元中的权重乘法（weightmultiplications）替换为考虑图拓扑的卷积（convolutions），比如图卷积或扩散卷积。</li></ol><p><strong>主要贡献：</strong>利用注意力机制来生成动态变化的邻接矩阵，再将其用于RNN 来捕获时空依赖关系。</p><blockquote><p>这就忽略了语义关系吧？图卷积和扩散卷积是能产生邻接矩阵吗？</p></blockquote><p> </p><h2 id="problem-definition">PROBLEM DEFINITION</h2><p>为了更好地帮助读者理解，作者还详细地说明了如何将现实问题转换为符号来表达。</p><h3 id="time-series">Time Series</h3><p>假设有 <span class="math inline">\(N\)</span> 个实体，<spanclass="math inline">\(T\)</span>个时间戳，那么每个实体在某一时间戳的状态被表示为 <spanclass="math inline">\(x_t\)</span> 。对于第 <spanclass="math inline">\(i\)</span> 个实体，则它的所有状态构成一个时间序列<spanclass="math inline">\(TS^{(i)}=&lt;x_{1}^{(i)},...,x_{T}^{(i)}&gt;\)</span>，其中<span class="math inline">\(x_t\)</span> 是一个 <spanclass="math inline">\(K\)</span>维的特征向量。至此，我们将能够表示所有的时间序列：<spanclass="math inline">\(TS^{(1)},...,TS^{(N)}\)</span> 。</p><p><strong>任务要求：</strong>给定所有实体的历史状态，预测所有实体的未来状态。历史状态和未来状态都用滑动窗口window 来表示：</p><ul><li>历史状态是指前 <span class="math inline">\(L\)</span> 个时间戳，即<span class="math inline">\([t_{a-L+1},t_{a}]\)</span> 窗口</li><li>未来状态是指后 <span class="math inline">\(P\)</span> 个时间戳，即<span class="math inline">\([t_{a+1},t_{a+P}]\)</span> 窗口</li></ul><p>因此又被称为 p-step ahead forecasting 问题。</p><p> </p><h3 id="graph-signals">Graph Signals</h3><p>建立有向图 <span class="math inline">\(G=(V,E)\)</span>，其中 <spanclass="math inline">\(v \in V\)</span> 表示每个实体，且有 <spanclass="math inline">\(|V|=N\)</span> 。用邻接矩阵 <spanclass="math inline">\(E \in R^{N \times N}\)</span>来表示所有的边，如果实体 <span class="math inline">\(i\)</span> 和实体<span class="math inline">\(j\)</span> 之间有边则 <spanclass="math inline">\(E[i,j]=1\)</span> 。</p><blockquote><p>显然，这个边不包含任何的语义信息，需要调整后才能用到知识图谱中。</p></blockquote><p>对于某一时刻 <spanclass="math inline">\(t\)</span>，所有实体向量的集合为 <spanclass="math inline">\(X_{t} \in R^{N \timesK}=[x_{t}^{(1)},...,x_{t}^{(N)}]^{T}\)</span>，其中 <spanclass="math inline">\(x_t\)</span> 是一个 <spanclass="math inline">\(K\)</span>维的特征向量。下标表示是哪一个时刻，上标表示是哪一个实体。</p><p>从而有：</p><ul><li>历史状态输入为 <span class="math inline">\(X^{(L)}=[X_{t-L+1},...,X_{t}]\)</span></li><li>未来状态输入为 <span class="math inline">\(X^{(P)}=[X_{t+1},...,X_{t+P}]\)</span></li></ul><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GARNN2.png"alt="(a)为X_{t}   (b)为X^{(L)}   其中维度K=3" /><figcaption aria-hidden="true">(a)为<spanclass="math inline">\(X_{t}\)</span>   (b)为<spanclass="math inline">\(X^{(L)}\)</span>   其中维度<spanclass="math inline">\(K=3\)</span></figcaption></figure><p> </p><h2 id="ga-rnn">GA-RNN</h2><p>Graph Attention RNN：分为 attention 部分和 RNN 部分，并且使用的是encoder-decoder 架构。</p><h3 id="spatial-modeling">Spatial Modeling</h3><p>为在集合 <span class="math inline">\(NB(i)=\left \{j|E[i,j]=1\right\}\cup\left \{i\right \}\)</span> 中的实体计算 attention score，即实体<span class="math inline">\(i\)</span> 的（1-hop）邻居和 <spanclass="math inline">\(i\)</span> 自己。</p><p>以下是计算流程：假设实体 <span class="math inline">\(i\)</span> 在<span class="math inline">\(t\)</span> 时刻的状态为 <spanclass="math inline">\(x_{t}^{(i)}\in R^{K}\)</span>，实体 <spanclass="math inline">\(j\)</span> 是集合 <spanclass="math inline">\(NB(i)\)</span>中的一个元素。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GARNN3.png"alt="attention score" /><figcaption aria-hidden="true">attention score</figcaption></figure><p><span class="math inline">\(A_{t}\)</span> 是 <spanclass="math inline">\(t\)</span> 时刻的邻接矩阵，<spanclass="math inline">\(A_{t}[i,j]\)</span> 是实体 <spanclass="math inline">\(i\)</span> 和 <spanclass="math inline">\(j\)</span> 之间的边，可见 edge被定义为实体之间的相关度。这里采用的计算相似度的方法应该是Additive，和之前看的 GGAM 模型是一样的。</p><blockquote><p>区别在于 GGAM 还考虑了关系 r，而 GA-RNN 和 GAT都只考虑了两个实体之间的相似度。</p></blockquote><p>维度说明：</p><ul><li><span class="math inline">\(x_{t}^{(i)}\in R^{K}\)</span></li><li><span class="math inline">\(W\in R^{F\times K}\)</span></li><li><span class="math inline">\(v\in R^{2F\times 1}\)</span></li></ul><p><strong>multi-head attention</strong></p><blockquote><p>Motivated by [17], we observe that stacking multiple attentionnetworks, a.k.a., attention heads, is beneficial since each attentionnetwork can specialise on capturing different interactions.</p><p>多头注意力机制可以用来捕获不同的关系。</p></blockquote><p>同样作者也用了多头注意力机制，并且最后用的是 averaging：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GARNN4.png"alt="multi-head attention score" /><figcaption aria-hidden="true">multi-head attention score</figcaption></figure><p>维度说明：</p><ul><li><span class="math inline">\(x_{t}^{(i)}\in R^{K}\)</span></li><li><span class="math inline">\(W^{(c)}\in R^{F\times K}\)</span></li><li><span class="math inline">\(v^{(c)}\in R^{2F\times 1}\)</span></li></ul><p>GA-RNN 等并没有像 Transformer一样把一个单头劈成多个头，而是保持和单头时的维度一致。</p><p>根据上述操作，我们可以分别计算出 <spanclass="math inline">\(t\)</span> 时刻的两个邻接矩阵 <spanclass="math inline">\(A_{t}^{(in)}\)</span> 和 <spanclass="math inline">\(A_{t}^{(out)}\)</span>，分别对应流入的流量和流出的流量。此外，虽然<span class="math inline">\(W^{(c)}\)</span> 和 <spanclass="math inline">\(v^{(c)}\)</span> 是固定的，但是由于 <spanclass="math inline">\(x_{t}\)</span>随时间的变化而变化，因此邻接矩阵也是动态变化的。</p><p> </p><h3 id="spatio-temporal-modeling">Spatio-Temporal Modeling</h3><p>定义一个扩散卷积操作：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GARNN5.png"alt="convolution operation" /><figcaption aria-hidden="true">convolution operation</figcaption></figure><p>暂时只涉及下图中的三个变量：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GARNN6.png"alt="RNN unit" /><figcaption aria-hidden="true">RNN unit</figcaption></figure><blockquote><p>目前的理解：<span class="math inline">\(\Theta\)</span>是一个卷积核，又称 filter 过滤器。本文的 <spanclass="math inline">\(\Theta\)</span> 是一个张量，有 2 个通道，<spanclass="math inline">\(\theta_{k,h,1}\)</span> 和 <spanclass="math inline">\(\theta_{k,h,2}\)</span> 是 <spanclass="math inline">\(\Theta\)</span> 对应位置上的权重。而 attention计算出来的 <span class="math inline">\(A_{t}^{out}\)</span> 和 <spanclass="math inline">\(A_{t}^{in}\)</span>对卷积核的权重起到动态调整的作用，因此被称为动态卷积。</p></blockquote><p>本文使用的是 RNN 的变形体GRU，并且还用上面定义的扩散卷积操作替换了原本的线性变换操作：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GARNN8.png"alt="GRU unit" /><figcaption aria-hidden="true">GRU unit</figcaption></figure><p> </p><h3 id="loss-function">Loss Function</h3><p>损失函数如下：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GARNN7.png"alt="loss function" /><figcaption aria-hidden="true">loss function</figcaption></figure><p>参数说明：</p><ul><li><span class="math inline">\(Y\)</span> 代表有 <spanclass="math inline">\(Y\)</span> 个样本实例（training instances）</li><li><span class="math inline">\(N\)</span> 代表有 <spanclass="math inline">\(N\)</span> 个实体</li><li><span class="math inline">\(P\)</span> 代表预测的 <spanclass="math inline">\(P\)</span> 个未来状态</li><li><span class="math inline">\(x_{p,y}\)</span> 代表真实值 GT</li><li><span class="math inline">\(\widehat{x}_{p,y}\)</span> 代表 GA-RNN的预测值</li></ul><blockquote><p>每个训练实例都是一个包含输入数据和对应标签的样本，按个人理解就是包含了一组样本点。</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>RNN</tag>
      
      <tag>Attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN、CNN、Attention 三者比较</title>
    <link href="/2023/12/01/RNN&amp;CNN&amp;Attention/"/>
    <url>/2023/12/01/RNN&amp;CNN&amp;Attention/</url>
    
    <content type="html"><![CDATA[<h1 id="rnncnnattention-三者比较">RNN、CNN、Attention 三者比较</h1><p><strong>参考博客</strong></p><p>知乎：<a href="https://zhuanlan.zhihu.com/p/47063917">Attention机制详解（一）</a></p><p>知乎：<ahref="https://www.zhihu.com/question/288081659/answer/1222002868">Attention跟一维卷积的区别是啥？</a></p><p>B 站：<ahref="https://www.bilibili.com/video/BV1Xp4y1b7ih?p=3&amp;vd_source=0c1ade3a93fcb106c44279021f3cf693">台大李宏毅21 年机器学习课程 self-attention 和 transformer</a></p><p> </p><h2 id="rnn-v.s.-attention">RNN v.s. Attention</h2><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RNN.png"alt="RNN vs Self-attention" /><figcaption aria-hidden="true">RNN vs Self-attention</figcaption></figure><p><strong>个人总结</strong></p><table><thead><tr class="header"><th>RNN</th><th>Self-attention</th></tr></thead><tbody><tr class="odd"><td>只能关注之前的信息，但双向 RNN 已解决该问题</td><td>能够关注全部的信息</td></tr><tr class="even"><td>存在长距离依赖问题，难以捕捉古早的信息</td><td>不存在长距离依赖问题，容易捕捉古早的信息</td></tr><tr class="odd"><td>不是并行化</td><td>是并行化</td></tr></tbody></table><blockquote><p>Attention机制，简单说就是从特征中学习或者提取出<strong>权重分布</strong>，再拿这个权重分布施加在原来的特征之上，改变原有特征的分布，增强有效特征抑制无效的特征或者是噪音。</p></blockquote><p> </p><h2 id="cnn-v.s.-attention">CNN v.s. Attention</h2><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/%E6%AF%94%E8%BE%83.png"alt="RNN v.s. CNN v.s. Self-attention" /><figcaption aria-hidden="true">RNN v.s. CNN v.s.Self-attention</figcaption></figure><p><strong>Context Window</strong></p><p>一维卷积需要指定窗口的大小，比如上图中的卷积每次只能看到 3 个词。而Self-attention的窗口是无限的，即序列的长度是多少，窗口的大小就是多少。这是因为Self-attention 的权重计算涉及到了序列里的所有词。如上图所示，句子有 5个词，那么窗口的大小就是 5 。因此，一维卷积是“local”的，而Self-attention 是“global”的。</p><p><strong>Time Complexity</strong></p><p>Time Complexity 也是取决于窗口的大小。由于一维卷积只能看到 <spanclass="math inline">\(k\)</span> 个词，如果序列长度为 <spanclass="math inline">\(n\)</span> 的话，那么时间复杂度就是 <spanclass="math inline">\(kn\)</span> 。而对于Self-attention，由于每个词的权重计算都要考虑到所有的词，所以复杂度就是<span class="math inline">\(n^2\)</span> 。</p><p><strong>Dynamic Weights</strong></p><p>和二维卷积一样，一维卷积的权重是不变的，即不会随着在序列中的位置的变化而改变。但是Self-attention 则不同，其在每个位置的权重都不同。通常，attention score是由 dot-product 计算出来的，即 <spanclass="math inline">\(Softmax(qk/\sqrt{d})\)</span> 。</p><p><strong>总结改进</strong></p><p>虽然 Self-attention 在两个方面都打败了一维卷积，但是 Self-attention的复杂度高，对长序列的建模效果没有那么好。以下是对二者的改进方向：</p><ul><li>对于一维卷积，改进动态权重和动态窗口大小</li><li>对于 Self-attention，限制无限窗口和进行层次化</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>词嵌入</tag>
      
      <tag>RNN</tag>
      
      <tag>Attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs</title>
    <link href="/2023/11/29/RSN/"/>
    <url>/2023/11/29/RSN/</url>
    
    <content type="html"><![CDATA[<h1id="learning-to-exploit-long-term-relational-dependencies-in-knowledge-graphs">Learningto Exploit Long-term Relational Dependencies in Knowledge Graphs</h1><p><strong>参考博客</strong></p><p>知乎：<a href="https://zhuanlan.zhihu.com/p/72527309">[论文笔记两篇]Distance-measurable KG embedding</a></p><p> </p><h2 id="abstract">Abstract</h2><p>基于三元组学习 KG 嵌入的问题如下：</p><ol type="1"><li><p>缺乏捕获实体之间的长期关系依赖的能力</p></li><li><p>缺乏传播实体之间的语义信息的能力，特别是跨 KG 时</p></li></ol><blockquote><p>长期关系依赖（long-term relational dependencies）</p></blockquote><p>因此，作者提出 RSN（recurrent skipping networks）：</p><ul><li><p>基于关系路径；反向关系</p></li><li><p>使用跳过机制（skipping mechanism）来传播信息</p></li><li><p>采用 RNN + residual learning 结构</p></li><li><p>有偏随机游走</p></li><li><p>NCE 产生负例</p></li></ul><p> </p><h2 id="preliminaries">Preliminaries</h2><p>本节讲述了一些预备知识。</p><p> </p><h3 id="relational-path">relational path</h3><p>关系路径就是一条包含实体和关系的链，并且实体和关系是交替出现的。关系路径被表示为<spanclass="math inline">\((x_1,x_2,...,x_T)\)</span>，其中要求链头是实体，并且<span class="math inline">\(T\)</span> 为奇数。若 <spanclass="math inline">\(t-1\)</span> 为奇数，那么 <spanclass="math inline">\((x_{t-1},x_{t},x_{t+1})\)</span>就是一个三元组。</p><p>下面是一条关系路径：</p><p><span class="math display">\[United\ Kingdom\to country^{-}\to Tim\Berners\ Lee\to emploer\to W3C\]</span></p><p>可以被表示为 <span class="math inline">\((United\Kingdom,country^{-},Tim\ Berners\ Lee, emploer,W3C)\)</span> 。</p><p> </p><h3 id="rnns">RNNs</h3><p>RNN 的缺点：</p><ul><li>不能区分 entity 和 relation，捕捉语义信息的能力较弱</li><li>忽视了知识图谱的基本表示单元——三元组</li></ul><p>对于 RNN 模型，现给定关系路径 <spanclass="math inline">\((x_1,x_2,...,x_T)\)</span>，其中 <spanclass="math inline">\(x_t\)</span> 是 <spanclass="math inline">\(d\)</span> 维向量。每个时间步 <spanclass="math inline">\(t\)</span> 产生的隐藏状态 <spanclass="math inline">\(h_t\)</span> 为：</p><p><spanclass="math display">\[h_t=tanh(W_hh_{t-1}+W_xx_t+b)\]</span></p><p>其中，<span class="math inline">\(h_t\)</span> 只包含了 <spanclass="math inline">\(h_{t-1}\)</span> 和 <spanclass="math inline">\(x_t\)</span> 的信息。在三元组 <spanclass="math inline">\((x_{t-1},x_{t},x_{t+1})\)</span> 中，<spanclass="math inline">\(x_{t-1}\)</span> 对于预测 <spanclass="math inline">\(x_{t+1}\)</span> 至关重要。虽然 <spanclass="math inline">\(h_{t-1}\)</span> 混合了 <spanclass="math inline">\(x_1,...,x_{t-1}\)</span>的所有信息，但是我们期望能保留一个纯粹的 <spanclass="math inline">\(x_{t-1}\)</span> 信息来进行预测。</p><p> </p><h3 id="residual-learning">residual learning</h3><p>假设输入是 <span class="math inline">\(x\)</span>，初始的映射是 <spanclass="math inline">\(F(x)\)</span>，Ground Truth 是 <spanclass="math inline">\(y\)</span> 。相较于直接让 <spanclass="math inline">\(F(x)\)</span> 去拟合 <spanclass="math inline">\(y\)</span>，去拟合 <spanclass="math inline">\(y-x\)</span> 会更加容易。因为考虑极端情况 <spanclass="math inline">\(y=x\)</span>，让 <spanclass="math inline">\(F(x)\)</span> 去拟合 <spanclass="math inline">\(0\)</span> 比去拟合 <spanclass="math inline">\(y\)</span> 容易。</p><p> </p><h2 id="rsn-model">RSN Model</h2><h3 id="skipping-mechanism">Skipping Mechanism</h3><p>RSN 模型如下图所示：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RSN1.png"alt="RSN with a 2-hop relational path" /><figcaption aria-hidden="true">RSN with a 2-hop relationalpath</figcaption></figure><p>跳过操作的描述如下：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RSN2.png"alt="Skipping Mechanism" /><figcaption aria-hidden="true">Skipping Mechanism</figcaption></figure><p>即在 RNN 的基础上，增加了 <spanclass="math inline">\(x_{t-1}\)</span> 的信息。注意，仅仅是对关系对应的<span class="math inline">\(h_t\)</span> 增加了 <spanclass="math inline">\(x_{t-1}\)</span> 的信息，没有对实体的 <spanclass="math inline">\(h_t\)</span> 增加。</p><blockquote><p>我猜测，本文主要是预测 Object 而不是 Relation，所以这样区别对待。</p></blockquote><p> </p><h3 id="residual-learning-1">Residual Learning</h3><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RSN3.png"alt="residual" /><figcaption aria-hidden="true">residual</figcaption></figure><p>对于三元组 <spanclass="math inline">\((x_{t-1},x_{t},x_{t+1})\)</span>，其中 <spanclass="math inline">\(x_t\)</span> 是 Relation，需要预测的是 <spanclass="math inline">\(x_{t+1}\)</span> 。首先输入 <spanclass="math inline">\(x_t\)</span> 和 <spanclass="math inline">\(h_{t-1}\)</span> 得到 <spanclass="math inline">\(h_t\)</span>，然后使用 <spanclass="math inline">\(h_t\)</span> 和 <spanclass="math inline">\(x_{t-1}\)</span> 去预测 <spanclass="math inline">\(x_{t+1}\)</span> 。换句话说，<spanclass="math inline">\(F_1(h_{t-1},x_t)\)</span> 要去拟合 <spanclass="math inline">\(h_t-x_{t-1}\)</span>，<spanclass="math inline">\(F_2(h_{t},x_{t-1})\)</span> 要去拟合 <spanclass="math inline">\(x_{t+1}-x_{t-1}\)</span> 。</p><p>总结为下表：</p><table><thead><tr class="header"><th>初始的映射</th><th>希望的映射</th><th>拟合的残差</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(F_1(h_{t-1},x_t)\)</span></td><td><span class="math inline">\(H_1(h_{t-1},x_t)\)</span> 或写作 <spanclass="math inline">\(h_t\)</span></td><td><span class="math inline">\(H_1(h_{t-1},x_t)-x_{t-1}\)</span></td></tr><tr class="even"><td><span class="math inline">\(F_2(h_{t},x_{t-1})\)</span></td><td><span class="math inline">\(H_2(h_{t},x_{t-1})\)</span> 或写作 <spanclass="math inline">\(x_{t+1}\)</span></td><td><span class="math inline">\(H_2(h_{t},x_{t-1})-x_{t-1}\)</span></td></tr></tbody></table><p> </p><h3 id="biased-random-walks">Biased random walks</h3><p>我们希望得到一条深的、跨 KG 的关系路径。</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RSN7.png"alt="random walk" /><figcaption aria-hidden="true">random walk</figcaption></figure><p><strong>depth bias</strong></p><p>假设当前实体为 <span class="math inline">\(e_i\)</span>，上一跳实体为<span class="math inline">\(e_{i-1}\)</span>，而 <spanclass="math inline">\(e_i\)</span> 的 1-hop 邻居是下一跳的候选实体 <spanclass="math inline">\(e_{i+1}\)</span> 。我们希望 <spanclass="math inline">\(e_{i+1}\)</span> 尽可能的离 <spanclass="math inline">\(e_{i-1}\)</span> 远，因此根据 <spanclass="math inline">\(e_{i-1}\)</span> 和 <spanclass="math inline">\(e_{i+1}\)</span> 之间的距离来计算深度偏好：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RSN4.png"alt="depth bias" /><figcaption aria-hidden="true">depth bias</figcaption></figure><p><span class="math inline">\(\alpha\in(0,1)\)</span>是一个控制深度偏好的超参数，由于我们喜欢更深的路径，因此 <spanclass="math inline">\(\alpha&gt;0.5\)</span> 。</p><p><strong>cross-KG bias</strong></p><p>同理，我们希望 <span class="math inline">\(e_{i+1}\)</span> 和 <spanclass="math inline">\(e_{i-1}\)</span> 不在同一张 KG，因此根据 <spanclass="math inline">\(e_{i-1}\)</span> 和 <spanclass="math inline">\(e_{i+1}\)</span> 是否处于同一 KG 来计算跨 KG偏好：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RSN5.png"alt="cross-KG bias" /><figcaption aria-hidden="true">cross-KG bias</figcaption></figure><p><span class="math inline">\(\beta\in(0,1)\)</span> 是一个控制跨 KG偏好的超参数，由于我们喜欢跨 KG 的路径，因此 <spanclass="math inline">\(\beta&gt;0.5\)</span> 。</p><p>综上，总的偏好如下：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RSN6.png"alt="bias" /><figcaption aria-hidden="true">bias</figcaption></figure><p> </p><h3 id="type-based-nce">Type-based NCE</h3><p>损失函数基于 NCE：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RSN8.png"alt="loss function" /><figcaption aria-hidden="true">loss function</figcaption></figure><p>若下一个预测的是实体，则从实体的噪声概率分布中提取负样本；若下一个预测的是关系，则从关系的噪声概率分布中提取负样本。通过这种方式，负采样的候选集被压缩，不适用的负样本也可以被避免。</p>]]></content>
    
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
      <tag>RNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>知识图谱嵌入</title>
    <link href="/2023/11/26/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%B5%8C%E5%85%A5/"/>
    <url>/2023/11/26/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E5%B5%8C%E5%85%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="知识图谱嵌入">知识图谱嵌入</h1><p><strong>参考博客</strong></p><p>知乎：<a href="https://zhuanlan.zhihu.com/p/450869761">论文浅尝 |PairRE: 通过成对的关系向量实现知识图谱嵌入</a></p><p>知乎：<a href="https://zhuanlan.zhihu.com/p/459413650">知识总结:知识图谱嵌入技术简析</a></p><p> </p><h2 id="研究目的">研究目的</h2><p>知识图谱因其在问答、语义解析和命名实体消歧等任务取得了良好的效果而受到广泛关注，而大部分知识图谱都存在不全和缺失实体链路的问题，所以需要进行链路预测（LinkPrediction）和知识图谱补全（Knowledge Graph Completion）。</p><p><strong>知识图谱补全目前主要被抽象成一个链路预测问题</strong>，即预测出三元组中缺失的部分，如：(?,r, v_t) 头实体预测、(v_h, r, ?) 尾实体预测和 (v_h, ?, v_t)关系预测。其中问号表示需要预测的部分，而另外两个部分是已知的。</p><blockquote><p>源自：<ahref="https://jeit.ac.cn/cn/article/doi/10.11999/JEIT210321?viewType=HTML">基于双曲图注意力网络的知识图谱链路预测方法</a></p></blockquote><p><strong>知识图谱嵌入（Knowledge GraphEmbedding）是解决知识图谱补全问题的重要方法之一</strong>，它通过将知识图谱中的实体和关系嵌入到连续向量空间，从而在方便计算的同时保留知识图谱中的结构信息。</p><blockquote><p>嵌入到连续向量空间=嵌入到低维空间</p></blockquote><p>从基于距离的方法 TransE 到目前的效果最好的RotatE，在知识图谱补全任务上效果持续提升。</p><p> </p><h2 id="主要问题">主要问题</h2><p>在知识图谱嵌入中，有两个主要问题受到广泛关注。</p><p><strong>问题一：如何解决 1-to-N、N-to-1 和 N-to-N的复杂关系表示问题。</strong></p><p>比如 1-to-N 的关系：基于距离的方法中，三元组 (StevenSpielberg,DirectorOf, ?) 的补全要求在经过关系变换 DirectorOf 后，Jaws 和JurassicPark 等实体能够距离 StevenSpielberg更接近，但是使所有这类实体具有不同的表示是很困难的。</p><p><strong>问题二：如何通过已有的三元组学习和推断关系模式。</strong></p><p>常见的关系模式：</p><ul><li>对称（Symmetry）关系 IsSimilarTo</li><li>非对称（Antisymmetry）关系 FatherOf</li><li>逆向（Inverse）关系 PeopleBornHere 和 PlaceOfBirth</li><li>组合（Composition）关系 mother’s father is grandpa</li></ul><p> </p><h2 id="模型分类">模型分类</h2><p>知识图谱嵌入模型大致可以分为三类：</p><ul><li>基于距离的模型（Distance-based Models）</li><li>双线性模型（Bilinear Models）</li><li>神经网络模型（Neural Network Models）</li></ul><p> </p><h1 id="embedding">Embedding</h1><h2 id="含义">含义</h2><p>由于模型不懂人话且只认识数字，因此需要把人话转换成数字表示，这就是embedding，即词嵌入。</p><p>换句话说，嵌入就是指把一个字或者一个词转换成一组向量来表示，向量里面的每个参数表示在不同维度上的值，这些值是通过训练得到的。转换后模型就能看懂了，并且能通过该向量中的参数学习到字与字之间的关系。</p><p> </p><h2 id="应用">应用</h2><p><strong>Word Embedding</strong></p><p>把单词 w 映射到向量 x。如果两个词的原意接近，比如 coronavirus 和covid，那么它们映射后得到的两个词向量 x_1 和 x_2之间的欧式距离应该很小。</p><p><strong>User Embedding</strong></p><p>在推荐系统中，使用一个向量来表示一个用户，即把用户 ID 映射到向量x。如果两个用户的行为习惯接近，那么他们映射后得到的两个向量 x_1 和 x_2之间的欧式距离应该很小。</p><p><strong>Graph Embedding</strong></p><p>把图中的每个节点映射成一个向量x。如果图中两个节点接近，比如它们的最短路径很小，那么它们映射后得到的两个词向量x_1 和 x_2 之间的欧式距离应该很小。</p><p> </p><h2 id="图嵌入">图嵌入</h2><p>在知识图谱嵌入中，使用一个<strong>低维的、稠密的</strong>向量去表示图中的节点和边的关系，即该向量可以反映图中的结构。</p><p>图嵌入的好处就在于可以使用向量表示数据，通过输入到机器学习模型中，就可以解决一些的具体问题。比如：如果在原网络中两个点的结构类似，那么这两个点表示成的向量也应该类似。</p>]]></content>
    
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Representing Text for Joint Embedding of Text and Knowledge Bases</title>
    <link href="/2023/11/24/CONV/"/>
    <url>/2023/11/24/CONV/</url>
    
    <content type="html"><![CDATA[<h1id="representing-text-for-joint-embedding-of-text-and-knowledge-bases">RepresentingText for Joint Embedding of Text and Knowledge Bases</h1><h2 id="abstract">Abstract</h2><p>学习表示同一连续潜在空间（continuous latentspace）中的文本关系和知识库关系的模型能够对两种关系进行联合推理，并获得较高的知识库补全精度。在本文中，我们提出了一个模型来捕获文本关系的组成结构，并联合优化实体、知识库和文本关系表示。与不共享具有公共子结构的文本关系参数的模型相比，本文提出的模型显著提高了性能。</p><p> </p><h2 id="新术语">新术语</h2><h3 id="连续表示">连续表示</h3><p>连续表示（continuous representations）是指用来表示对象、概念或数据的向量或矩阵形式的特征。这些特征是连续值，而不是离散值。</p><h3 id="潜在特征">潜在特征</h3><p>潜在特征（latentfeatures）是指在学习过程中未直接观测到的特征。这些特征不是通过手动设计，而是由模型自动学习的。在许多深度学习模型中，潜在特征常常是模型通过训练数据学到的高层抽象表示，反映了数据中的重要特征和关系。</p><h3 id="文本提及">文本提及</h3><p>文本提及（textualmention）是指在文本中提到某个实体或概念。这种提及可能涉及到<strong>命名实体、关键词、短语</strong>等，通常是为了<strong>指代</strong>文本中讨论的具体内容。例如，在文本中提到了公司的名称、人物、地点等都可以被认为是文本提及。</p><h3 id="文本关系">文本关系</h3><p>文本关系（textualrelation）是指在文本中不同元素之间的关系。这种关系可以是语法上的关系，也可以是语义上的关系。以下是一些文本关系的示例：</p><ul><li>同义关系：汽车和轿车。</li><li>上下位关系：橡树是树的上下位词。</li><li>时序关系：早上起床发生在吃早餐之前。</li><li>因果关系：下雨导致街道湿润。</li></ul><h3 id="远程监督">远程监督</h3><p>远程监督（Distantsupervision）是一种在自然语言处理（NLP）和信息检索领域中的方法，用于生成标注数据以训练模型。这种方法的核心思想是通过利用现有的知识库或结构化数据来为非结构化文本生成标签，而无需手动标注大量的训练数据。</p><p> </p><h2 id="introduction">Introduction</h2><p>著名的知识库（KB）：Freebase、YAGO、DBPedia，但是这些 KB仍然不完整。</p><p>这推动了自动推导新事实以扩展手动构建的知识库的研究，使用来自现有知识库的信息、实体的文本提及以及表格和网页形式等<strong>半结构化数据</strong>。</p><p>在本文中，我们联合学习知识库和文本关系的连续表示。</p><p>文本关系（textual relation）表示了句子中实体之间的关系。</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/TR.png"alt="KB和textual relation" /><figcaption aria-hidden="true">KB和textual relation</figcaption></figure><p>Riedel 等人使用词汇化的依赖路径来表示两个实体之间的文本提及。</p><p>通过两个实体之间的词汇化依赖路径表示实体对的每个文本提及</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/TR2.png"alt="表示规则" /><figcaption aria-hidden="true">表示规则</figcaption></figure><p>我们假设知识库使用 RDF 三元组表示，形式为 (subject, predicate,object)，其中主语和宾语是实体，谓词是关系。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/notation.png"alt="符号定义（s: subject o:object）" /><figcaption aria-hidden="true">符号定义（s: subjecto:object）</figcaption></figure><p>训练任务是预测没有出现在 KB中的新关系，即针对给出的询问，模型要能够给出候选的实体：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/task.png"alt="训练任务" /><figcaption aria-hidden="true">训练任务</figcaption></figure><p>局部封闭世界假设（local closed world assumption）</p><p>“是说如果没有见过就认为它不存在，比如在知识图谱中之前没有新冠这个词，那就不应该被添加进来。反之就是开放世界假定，什么都可以加进来，但是会有准确率问题。”</p><blockquote><p>知乎：https://www.zhihu.com/question/380560296</p></blockquote><p>认为我们的图谱是部分完备的。比如假设我们的图谱中已经包含了所有航空公司的所有航线，在所有这些航线中都没有Viña del Mar 和 Arica之间的航班，那么我们就认为这两地之间确实没有航班。</p><blockquote><p>博客：https://rogerspy.github.io/2021/05/29/kg-survey-3/</p></blockquote><p>本文的符号定义：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/notation2.png"alt="符号定义2" /><figcaption aria-hidden="true">符号定义2</figcaption></figure><p> </p><h2 id="models-for-kb-completion">Models for KB completion</h2><h3 id="basic-models">Basic Models</h3><p>F 和 E 用于包含 KB+text 的图谱，DISTMULT 用于只包含 KB relation的图谱。</p><p>model F：</p><ul><li>分别为实体对 (e_s, e_o) 和关系 r 学习 K 维潜在特征</li><li>score function = v(r) 点积 v(e_s, e_o)</li><li>因此，subject 和 object 相同的实体对之间不会共享参数</li></ul><p>model E：</p><ul><li>分别为实体 e_s 和 e_o 学习 K 维潜在特征</li><li>为关系 r 学习 2 个 K 维潜在特征：v(r_s) 和 v(r_o)</li><li>score function = v(r_s) 点积 v(e_s) + v(r_o) 点积 v(e_o)</li><li>模型预测候选实体时不依赖实体，而是只依赖关系 r</li></ul><p>DISTMULT：</p><ul><li>分别为实体 e_s、e_o 和关系 r 学习 K 维潜在特征</li><li>score function = v(r) 点积 (v(e_s) ○ v(e_o))</li><li>subject 和 object 相同的实体对之间会共享参数</li><li>模型预测候选实体时不依赖关系 r，而是只依赖实体</li></ul><p>以下是上述三种模型的参数数量比较：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/CONV3.png"alt="参数数量" /><figcaption aria-hidden="true">参数数量</figcaption></figure><p>model F 需要 KN_e^2+KN_r个参数，因为它是为实体对生成潜在特征的，而实体总数为N_e，因此根据排列组合可知有 N_e^2 个实体对。</p><p><strong>缺点：</strong>在这些基本模型中，KB 和文本关系（textualrelation）会被统一处理，即每个文本关系都会得到各自的 K维的潜在特征。按照这样的处理方式，当加入文本关系来训练模型的时候，关系数量R会大幅增加。此外，上述三个模型只会在训练时使用文本关系，在预测时则只会使用实体和KB 中的关系。</p><p> </p><h2 id="conv">CONV</h2><p>当前的模型孤立地看待每一个文本关系（textualrelation），从而使每一个文本关系都有独属于自己的潜在特征（latentfeature）。然而，许多文本关系之间只是一些些单词或者依赖弧不同。比如，下表展示了一些文本模式，它们都对应着KB 中的 person/organizations_founded 关系：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/table.png"alt="文本模式" /><figcaption aria-hidden="true">文本模式</figcaption></figure><p>虽然某些依赖路径出现得非常频繁（12次），但是许多与它非常类似的依赖路径只被使用了一次。</p><p>我们把词汇化的依赖路径视为单词序列和有向依赖弧，再对其使用 CNN。</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/CONV1.png"alt="模型" /><figcaption aria-hidden="true">模型</figcaption></figure><p>变量定义：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/CONV2.png"alt="变量定义" /><figcaption aria-hidden="true">变量定义</figcaption></figure><p>本文使用的 score function 如下：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/CONV4.png"alt="score function" /><figcaption aria-hidden="true">score function</figcaption></figure><p>Neg(e_s, r, ?) 是指不是三元组 (e_s, r, ?) 中的 object的实体的集合。</p><p>需要注意的是，基于边际损失函数（margin-based）和似然损失函数（log-likelihood）都容易受到潜在的假阴性（falsenegative）样本选择噪声的影响。训练损失函数的实证比较将是有趣的。</p><p>本文使用的损失函数如下：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/CONV5.png"alt="loss function" /><figcaption aria-hidden="true">loss function</figcaption></figure><p>本文使用的损失函数如下：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/CONV6.png"alt="loss function" /><figcaption aria-hidden="true">loss function</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>词嵌入</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GRAPH ATTENTION NETWORKS</title>
    <link href="/2023/11/22/GAT/"/>
    <url>/2023/11/22/GAT/</url>
    
    <content type="html"><![CDATA[<h1 id="graph-attention-networks">GRAPH ATTENTION NETWORKS</h1><p><strong>参考博客</strong></p><p>知乎：<a href="https://zhuanlan.zhihu.com/p/81350196">向往的GAT</a></p><p>知乎：<ahref="https://zhuanlan.zhihu.com/p/172254089">一文搞懂激活函数</a></p><p> </p><h2 id="graph-attentional-layer">GRAPH ATTENTIONAL LAYER</h2><p>GGAM 的变量定义和 GAT 的一模一样：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT7.png"alt="变量定义" /><figcaption aria-hidden="true">变量定义</figcaption></figure><p>N 表示节点的总数，F 是输入向量 h_i 的维度，F'是输出向量h_i'的维度。</p><p>升维的好处：“为了获得足够的表达能力，将输入特征转换为高维特征。”这需要一个可学习的线性变换W，其维度是 F'×F。</p><p> </p><h3 id="单头注意力">单头注意力</h3><p>下图描述了计算 relative attention value 的整个流程：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT11.png"alt="attention过程" /><figcaption aria-hidden="true">attention过程</figcaption></figure><p> </p><p>首先使用注意力机制 a 计算 absolute attention value：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT8.png"alt="absolute attention value" /><figcaption aria-hidden="true">absolute attention value</figcaption></figure><p>“In its most general formulation, the model allows every node toattend on every other node, <strong>dropping all structuralinformation</strong>.”</p><p>注意力机制允许每个节点都能关注到其它节点，但也因此丢弃了所有的位置信息。我猜，这就是为什么Transformer 使用了位置编码。</p><p>然后使用 Sotfmax 归一化 absolute attention value，得到 relativeattention value：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT9.png"alt="relative attention value" /><figcaption aria-hidden="true">relative attention value</figcaption></figure><p>这里采用的是 <strong>masked注意力机制</strong>，让每个节点只能关注到自己的邻居节点。具体来说，本文只关注直接邻居。</p><p>本文采用的是 FFN 来计算 absolute attentionvalue，因此公式可改写为：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT10.png"alt="具体的relative attention value" /><figcaption aria-hidden="true">具体的relative attentionvalue</figcaption></figure><p>FFN 的激活函数用的是 LeakyReLU。a 转置的维度为 1×2F'，Wh_i||Wh_j的维度为 2F'×1，因此运算结果为一个实数。</p><p>最后 h_i'仍然是自己邻居的加权求和：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT12.png"alt="输出向量" /><figcaption aria-hidden="true">输出向量</figcaption></figure><p>权值用的就是 relative attention value。此外，还用了一个非线性函数σ，其实就是一个激活函数。</p><p> </p><h3 id="多头注意力">多头注意力</h3><p>“To stabilize the learning process of self-attention, we have foundextending our mechanism to employ multi-head attention to be beneficial,similarly to <strong>Vaswani et al. (2017)</strong>.”模仿 Transformer使用多头注意力机制。</p><p>对于拼接版输出，h_i'的维度将会是 K*F'：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT13.png"alt="multi-head" /><figcaption aria-hidden="true">multi-head</figcaption></figure><p>本文采用的是 3 头，下图非常形象：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT14.png"alt="3-head attention" /><figcaption aria-hidden="true">3-head attention</figcaption></figure><p> </p><h3 id="leakyrelu">LeakyReLU</h3><p><imgsrc="https://3.bp.blogspot.com/-C9zZb522B6U/XHUf3eUr5UI/AAAAAAAAJj4/7Gn2ht4cJ5cOa7pAbTC_wiM8GZaBpoI4gCLcBGAs/w1200-h630-p-k-no-nu/ActivationFunctions.png" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</title>
    <link href="/2023/11/21/Attention-based%20KG/"/>
    <url>/2023/11/21/Attention-based%20KG/</url>
    
    <content type="html"><![CDATA[<h1id="learning-attention-based-embeddings-for-relation-prediction-in-knowledge-graphs">LearningAttention-based Embeddings for Relation Prediction in KnowledgeGraphs</h1><p><u>模型图莫名其妙；解码器是 CNN 所以没看</u></p><p> </p><p><strong>参考博客</strong></p><p>博客：<ahref="https://www.cnblogs.com/yanch01/p/note_Learning-Attention-based-Embeddings-for-Relation-Prediction-in-Knowledge-Graphs.html">论文阅读：LearningAttention-based Embeddings for Relation Prediction in KnowledgeGraphs(2019 ACL)</a></p><p> </p><h2 id="abstract">Abstract</h2><p>链路预测（link prediction）或称关系预测（relationprediction），对应到知识库补全（knowledge basecompletion）任务，它是通过“预测给定的三元组是否有效”来实现的。</p><p>我们观察到 CNN-based 的 KG向量分开地处理各个三元组，没有考虑到一个三元组的邻域中的信息。</p><p>为此，我们提出了一种 attention-based的特征向量，它能捕获任意实体的邻域中的实体和关系特征。</p><p>此外，我们还在模型中封装了<strong>关系簇和多跳关系</strong>。</p><p><strong>关系簇</strong>是指相似或相关的关系被组织成一个簇或集合。<strong>多跳关系</strong>是指获取两个实体之间的关系需要通过中间的实体和关系来实现跳转。</p><p> </p><h2 id="introduction">Introduction</h2><p>知识图（KG）将知识库（knowledge bases, KB）表示为有向图（directedgraph），其节点（node）和边（edge）分别表示实体和实体之间的关系。</p><p>主流的链路预测模型都是 knowledge embedding-based的模型，分为两派：<strong>Trans 系列和 CNN-based系列</strong>，但是它们都是分开处理各个三元组的，从而忽略了三元组邻域中的信息。</p><blockquote><p>基于知识嵌入的模型：将实体和关系映射到低维向量空间，并且使相似的实体和关系在向量空间中的距离较近。</p></blockquote><p>图注意力网络（graph attention networks,GAT）已被证明可以关注图谱中最相关的部分，即 1跳（1-hop）邻域中的节点（实体）特征。</p><p>作者的 Idea：</p><ul><li>捕捉多跳（multi-hop）关系</li><li>封装一个实体在不同关系中的角色</li><li>把语义相似的关系簇中的知识整合起来</li></ul><p>我们的模型<strong>将不同的权重（注意力）分配给邻域中的节点</strong>，并通过层来传播注意力。然而，随着模型深度的增加，远距离实体的贡献呈指数下降。为了解决这个问题，我们使用<strong>关系组合（relationcomposition）</strong>在 n-hop邻居之间引入辅助边，从而实现实体之间的知识流动。</p><p>我们采用编码器-解码器架构，其中我们的<strong>广义图注意力模型（generalizedgraph attention model）</strong>和 <strong>ConvKB（Nguyen et al.,2018）</strong>分别扮演编码器和解码器的角色。</p><p> </p><h2 id="our-approach">Our Approach</h2><h3 id="background">Background</h3><p>KG 中的符号和定义如下图所示：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/KG.png"alt="KG中的符号和定义" /><figcaption aria-hidden="true">KG中的符号和定义</figcaption></figure><p>一个知识图谱被表示为 G=(E, R)：</p><ul><li>G：graph</li><li>E：entity，这里表示实体（节点）的集合</li><li>R：relation，这里表示关系（边）的集合</li></ul><p>三元组 (e_s, r, e_o)，表示的是 e_s 和 e_o 之间的边r（但原文好像把下标打错了）。打分函数（score function）f 为每个三元组(e_s, r, e_o) 打分，判断该三元组是否有效。下图是一个链路预测的例子：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/Lp4.png"alt="链路预测举例" /><figcaption aria-hidden="true">链路预测举例</figcaption></figure><ul><li>根据 (CN, born_in, London) 和 (London, capital_of, UK) 来预测 (CN,nationality, UK)</li><li>根据 (CN, born_in, London) 和 (CN, brother_of, JN) 来预测 (JN,born_in, London)</li><li>根据 (CN, colleague, TH) 和 (CN, colleague, LD) 来预测 (TH,colleague, LD)</li></ul><p> </p><h3 id="gats">GATs</h3><p>GATs 学习为邻域中的节点分配权重，而不是像 GCNs（Graph convolutionalnetworks）一样平等地对待每一个节点。</p><p>下图对变量进行定义：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT1.png"alt="变量定义" /><figcaption aria-hidden="true">变量定义</figcaption></figure><p>输入向量为 X，经过注意力层后得到输出向量 X'。其中，X 和X'的每个元素表示的是每个实体的向量，N 是实体的总数。</p><p> </p><h4 id="单头注意力">单头注意力</h4><p>下图计算实体 e_i 和 e_j 之间的相似度：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT2.png"alt="计算相似度" /><figcaption aria-hidden="true">计算相似度</figcaption></figure><p>和 Transformer 中采用的操作一样，实体 e_i 和 e_j 的向量 x_i 和 x_j先和一个线性变换矩阵 W 相乘，再被送入注意力层中。如下图所示：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/T.png"alt="Transformer的multi-head attention" /><figcaption aria-hidden="true">Transformer的multi-headattention</figcaption></figure><p>公式（1）中的 a表示任意一种计算相似度的函数，比如：向量点乘、余弦相似度、曼哈顿编码等。最后得到的结果e_ij 就是实体 e_i 和 e_j 之间的相似度。相似度（attention score）衡量了边(e_i, e_j) 对源节点 e_i 的重要性。</p><p>下图是最终得到的 attention value：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT3.png"alt="注意力层的输出" /><figcaption aria-hidden="true">注意力层的输出</figcaption></figure><p>下图是 GATs 单头 attention 的整个计算过程：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT4.png"alt="attention value计算过程" /><figcaption aria-hidden="true">attention value计算过程</figcaption></figure><p>区别在于，Transformer是拿整个矩阵进行说明的，而本文是拿矩阵中的一个向量进行说明的。此外，在输出X'之前还加了一个非线性函数 σ。</p><p> </p><h4 id="多头注意力">多头注意力</h4><p>下图是采用多头注意力的结果：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT5.png"alt="multi-head attention" /><figcaption aria-hidden="true">multi-head attention</figcaption></figure><p>和 Transformer 一样采用多头注意力机制，我猜测还是把 W 劈成了多个W。</p><p>最终取各个头的结果的均值作为输出，而不是把它们拼接起来：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GAT6.png"alt="输出向量" /><figcaption aria-hidden="true">输出向量</figcaption></figure><p> </p><h3 id="ggam">GGAM</h3><p>“In KGs, entities play different roles depending on the relation theyare associated with.”</p><p>在知识图谱中，实体在不同关系中扮演着不同的角色，而 GATs忽略了关系的不同，平等地对待邻域中的每一个节点。</p><p>在 GGAM 中，注意力层的输入有两个：实体向量的矩阵 H 和关系向量的矩阵G，输出矩阵分别为 H'和 G'。如下图所示：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM.png"alt="变量定义" /><figcaption aria-hidden="true">变量定义</figcaption></figure><p>实体 e_i 不再是仅仅注意实体 e_j，而是注意自己所处的三元组 (e_i, r_k,e_j)。以下是三元组的向量表示：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM2.png"alt="三元组的向量表示" /><figcaption aria-hidden="true">三元组的向量表示</figcaption></figure><p>三元组向量 c_ijk 基于实体向量 h_i、实体向量 h_j 和关系向量 g_k的拼接，然后再乘了一个线性变换矩阵 W。</p><p> </p><h4 id="单头注意力-1">单头注意力</h4><p>名词说明：</p><ul><li>absolute attention value，相似度</li><li>relative attention value，权值</li></ul><p>下图中 b_ijk 表示这个三元组的重要性，即相似度（attentionscore）：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM3.png"alt="attention value" /><figcaption aria-hidden="true">attention value</figcaption></figure><blockquote><p>为什么要用一个 LeakyReLU？它能计算相似度吗？</p></blockquote><p>下图中 α_ijk 表示这个三元组所具有的权值：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM4.png"alt="权值矩阵的元素" /><figcaption aria-hidden="true">权值矩阵的元素</figcaption></figure><blockquote><p>为什么 R_ij 是个集合？难道 e_i 和 e_j 之间还有多条关系吗？</p></blockquote><p>下图是注意力层的输出结果：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM5.png"alt="单头输出结果" /><figcaption aria-hidden="true">单头输出结果</figcaption></figure><p>即新的实体向量表示 h_i'是它所处的三元组的加权和。</p><p> </p><h4 id="多头注意力-1">多头注意力</h4><p>本文还是采用了多头注意力机制，因此输出结果如下：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM6.png"alt="multi-head" /><figcaption aria-hidden="true">multi-head</figcaption></figure><p>作者还是没有采用拼接操作，而是对各个头的结果取的均值：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM7.png"alt="多头输出结果" /><figcaption aria-hidden="true">多头输出结果</figcaption></figure><p> </p><h4 id="残差连接">残差连接</h4><p>作者认为虽然学到了新的向量表示，但同时也丢了原始的向量表示，因此做一个残差连接以保留原始的信息：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/Res1.png"alt="残差连接" /><figcaption aria-hidden="true">残差连接</figcaption></figure><p>原始实体向量的维度为 T^i，而最后输出的向量的维度为 T^f，因此 H^i要乘一个线性变换矩阵 W^E，才能和 H^f 相加（原文好像把 H^i 打错了，H^i *W^E = H^t）。</p><p>整个模型如下图所示：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM10.png"alt="GGAM" /><figcaption aria-hidden="true">GGAM</figcaption></figure><p> </p><h4 id="多跳关系">多跳关系</h4><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/n-hop.png"alt="n-hop" /><figcaption aria-hidden="true">n-hop</figcaption></figure><p>在 Graph Attention Layer1，所有实体获得直接邻居的信息；在 GraphAttention Layer2，所有实体通过直接邻居获得间接邻居的信息。比如：U.S 通过Barack Obama 获得 Michelle Obama 的信息，通过 Washington D.C 获得 SamuelL. Jackson 的信息。这些信息都是 Barack Obama 和 Washington D.C 在 GraphAttention Layer1 获得的。</p><p>由此，U.S 能获得 2-hop邻居的信息，从而形成<strong>辅助边</strong>（由虚线表示）。如果模型有 n层，那么源节点就能获得 n-hop 邻居的信息。</p><p>“模型中，作者通过定义辅助关系，<strong>将边的定义拓展为有向路径</strong>，辅助关系的嵌入为<strong>有向路径中所有关系的嵌入之和</strong>，这样对于一个n 层的模型，在第 m 层就可以通过聚合相邻 m 跳的邻居计算出新嵌入。”</p><p> </p><h4 id="损失函数">损失函数</h4><p>本文采用的 score function 和 TransE 的一样，损失函数用的是<strong>hinge-loss</strong>：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM8.png"alt="loss function" /><figcaption aria-hidden="true">loss function</figcaption></figure><p> </p><h4 id="负例产生方法">负例产生方法</h4><p>本文采用的是传统的产生负例的方法：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/GGAM9.png"alt="negative sample" /><figcaption aria-hidden="true">negative sample</figcaption></figure><p> </p>]]></content>
    
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
    <link href="/2023/11/20/RotatE/"/>
    <url>/2023/11/20/RotatE/</url>
    
    <content type="html"><![CDATA[<h1id="rotate-knowledge-graph-embedding-by-relational-rotation-in-complex-space">RotatE:Knowledge Graph Embedding by Relational Rotation in Complex Space</h1><p><u>负采样看不懂</u></p><p> </p><p><strong>参考博客</strong></p><p>知乎：<ahref="https://zhuanlan.zhihu.com/p/387378387">小白必看：一文读懂推荐系统负采样</a></p><p>博客：<ahref="https://zhang-each.github.io/2021/08/08/reading9/">论文阅读笔记9：GAN&amp;KBGAN</a></p><p>源码：https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding</p><p> </p><h2 id="abstract">Abstract</h2><p>我们研究<strong>实体和关系的低维向量表示</strong>的目的就是为了实现好的<strong>链路预测</strong>。</p><p>本文提出了一种新的知识图片嵌入方法RotatE，该方法能够建模和推断各种关系模式（relationpatterns），包括对称和反对称关系、逆向关系、组合关系。</p><p>RotatE 将关系定义为，能够让源实体到目的实体的一个旋转。</p><p>此外，我们提出了一种新的<strong>自对抗负采样技术</strong>来有效地训练RotatE 模型。</p><p>之前产生负例的方法是，替换正例中的实体。</p><p> </p><h3 id="关系模式">关系模式</h3><p>尽管“国家-城市”、“国家-大学”、“洲-国家”都属于“位置包含”关系，但是每个模式都涉及到<strong>不同类型的头尾实体对</strong>，反映了在知识图谱中的多样性。因此，在处理这样的关系时，需要考虑到不同的模式，而不仅仅是一个通用的关系向量。这样可以更准确地捕捉到不同实体对之间的语义关系。</p><blockquote><p>这是 TransR 中的举例。</p></blockquote><p> </p><h2 id="introduction">Introduction</h2><blockquote><p>随便看看</p></blockquote><p>知识图谱的应用：</p><ul><li>问答（question-answering）</li><li>信息检索（information retrieval）</li><li>推荐系统（recommender systems）</li><li>自然语言处理（natural language processing）</li></ul><p> </p><h3 id="关系模式-1">关系模式</h3><ul><li>对称/反对称关系（symmetry/antisymmetry），如：婚姻/父子关系。如果在图谱中存在(x, 父子, y) 的三元组，则可以推断出 (y, 父子, x) 也成立。</li><li>逆向关系（inversion），如：上位词和下位词。如果在图谱中存在 (x,雇佣, y) 的三元组，可以推断出 (y, 被雇佣, x) 也成立。</li><li>组合关系（composition），如：我的母亲的丈夫是我的父亲。</li></ul><hr /><p>Q：为什么对称关系和反对称关系会被归为同一种关系模式？</p><p>A：因为它们具有相似的性质：如果一个关系在一个方向上成立，那么在相反的方向上也会成立或不成立。</p><p>具体来说：</p><ul><li>对称关系：如果 (x, r, y) 成立，则可推断出 (y, r, x)也成立，即两个实体之间的顺序不影响该关系的成立。</li><li>反对称关系：如果 (x, r, y) 成立，则可推断出 (y, r, x)不成立，即两个实体之间的顺序很重要。</li></ul><p>尽管对称关系和反对称关系在表现方式上有所不同，但它们都具有关系在一定程度上从一个实体到另一个实体的转换性质。因此，在关系模式的分类中将它们归为同一类，以便更好地进行建模和推断。</p><p> </p><h3 id="欧拉公式">欧拉公式</h3><p><strong>欧拉公式</strong></p><p>欧拉公式首次将三角函数与复指数函数关联起来，因其提出者莱昂哈德·欧拉而得名，公式如下：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/euler.png"alt="欧拉公式" /><figcaption aria-hidden="true">欧拉公式</figcaption></figure><p><strong>复平面</strong></p><p>下图是一个复平面，Im（Imaginary Axis）表示虚轴，Re（RealAxis）表示实轴。复平面中的二维单位向量 r_i，即 (cos(θ),sin(θ))，可以被写作复数形式cos(θ)+isin(θ)，也可以再进一步通过欧拉公式被写作复指数形式。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/euler2.png"alt="复平面" /><figcaption aria-hidden="true">复平面</figcaption></figure><p><strong>几何含义</strong></p><p>当一个复数向量 (x, y)乘以一个复数单位向量时，这个操作的几何含义是将该向量绕原点逆时针旋转（counterclockwiserotation）角度 θ 。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/euler3.png"alt="绕原点旋转" /><figcaption aria-hidden="true">绕原点旋转</figcaption></figure><p>为了更加直观地理解复数向量乘积的几何含义，我们接下来使用极坐标进行讨论。向量坐标如下：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/euler4.png"alt="极坐标" /><figcaption aria-hidden="true">极坐标</figcaption></figure><p>计算 h_i 和 r_i 的乘积结果，发现等于 h_i'：</p><p><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/euler5.png" /></p><p>从而验证了复数向量 h_i 乘以复数单位向量 r_i 的几何含义是复数向量 h_i绕原点逆时针旋转角度 θ 。</p><p>如果使用的是直角坐标系，则乘积结果为：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/euler6.png"alt="直角坐标系" /><figcaption aria-hidden="true">直角坐标系</figcaption></figure><p> </p><h3 id="rotate">RotatE</h3><p>基于欧拉公式，RotatE 把关系 relation 定义为使 h 能够到达 t的一个旋转。具体来说，t 是 h 和 r 的元素乘积（Hadamardproduct），定义如下：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RotatE.png"alt="RotatE对于r的定义" /><figcaption aria-hidden="true">RotatE对于r的定义</figcaption></figure><p> </p><h4 id="hadamard-product">Hadamard product</h4><p>Hadamardproduct，也称为元素乘积，是指两个相同维度的矩阵（或向量）对应位置上的元素相乘得到的新矩阵（或向量）。</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/HP.png"alt="Hadamard product" /><figcaption aria-hidden="true">Hadamard product</figcaption></figure><p> </p><h4 id="从关系模式的角度理解">从关系模式的角度理解</h4><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RotatE2.png"alt="从关系模式的角度理解" /><figcaption aria-hidden="true">从关系模式的角度理解</figcaption></figure><ul><li>对称关系：r 的每个 r_i 都等于 1 或 -1，即 r_i 的角度为 π的整数倍</li><li>逆向关系：r_1 和 r_2 共轭，即实数部分相同而虚数部分互为相反数</li><li>组合关系：r_3 可以由 r_1 通过 r_2 旋转而来</li></ul><p> </p><h2 id="our-method">Our Method</h2><h3 id="三种关系模式">三种关系模式</h3><p>文中对“对称/反对称关系”、“逆向关系”、“组合关系”的定义如下：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RP.png"alt="关系模式的定义" /><figcaption aria-hidden="true">关系模式的定义</figcaption></figure><p>作者说，目前没有一种模型可以同时很好地表示上述三种关系模式。</p><p> </p><h3 id="模型对比">模型对比</h3><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RotatE6.png"alt="模型对比" /><figcaption aria-hidden="true">模型对比</figcaption></figure><p> </p><h3 id="rotate-1">RotatE</h3><p>受到欧拉公式的启发，作者把关系 relation 定义为：一种能让头实体 h到达尾实体 t 的旋转，而且是一种 element-wise的旋转，即向量中的各个元素是分别旋转的（t_i=h_i*r_i）。具体定义如下：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RotatE3.png"alt="r是一种旋转" /><figcaption aria-hidden="true">r是一种旋转</figcaption></figure><p>其中的 C 代表“Complex”，即 h、t、r被映射到复数平面，而不是实数平面。约束 r_i 的模长为 1，使得 r_i是一个单位向量。实体向量与 r_i相乘，只会被影响到相位（phase），而不会被影响到幅度（magnitude）。</p><blockquote><p>相位表示向量的旋转角度，幅度表示向量的长度。</p></blockquote><p>从而 RotatE 的 distance function 为：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RotatE5.png"alt="distance function" /><figcaption aria-hidden="true">distance function</figcaption></figure><blockquote><p>Trans 系列都叫的是 score function。</p></blockquote><p> </p><h4 id="使用的范数">使用的范数</h4><p>本文使用的都是 L1-norm：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/Lp3.png"alt="Lp-norm" /><figcaption aria-hidden="true">Lp-norm</figcaption></figure><p> </p><h4 id="与-transe-相比">与 TransE 相比</h4><p>RotatE 能区分“对称”关系，而 TransE不能区分“对称”关系。所谓能够区分关系模式，就是指能够区分实体的不同。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/RotatE4.png"alt="TransE和RotatE translation是平移，rotation是旋转" /><figcaptionaria-hidden="true">TransE和RotatE translation是平移，rotation是旋转</figcaption></figure><p>“The reason is that in TransE, <strong>any symmetric relation will berepresented by a 0 translation vector</strong>. As a result, this willpush the entities with symmetric relations to be close to each other inthe embedding space.”</p><p>因此，TransE 会把“对称”关系中的头实体和尾实体误认为是同一个实体。而RotatE 不会遇到这种问题，头实体要通过关系旋转180°才能到达尾实体，因此它俩的向量表示不同，从而能够被区分。此外，不同的“对称”关系也能由不同的向量来表示，而不是被笼统地归纳为0 向量。</p><p> </p><h3 id="自对抗负采样">自对抗负采样</h3><blockquote><p>负采样算法的本质就是基于某些方式来设置或调整负采样时的采样分布。</p></blockquote><p>本文采用的损失函数：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/NS.png"alt="loss function" /><figcaption aria-hidden="true">loss function</figcaption></figure><p>传统的负采样方法就是随机替换三元组中的 h 或者 t来得到一个负样本，但这种方式产生的负样本可能太平凡了，很容易被模型判断出来是错的。我们训练模型就是想让它学会如何辨别一个三元组是不是合理的，而如果三元组太容易被辨别出来，训练就没有效果了。</p><p>因此，作者提出的自对抗负采样：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/NS2.png"alt="self-adversarial negative sampling" /><figcaption aria-hidden="true">self-adversarial negativesampling</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Learning Entity and Relation Embeddings for Knowledge Graph Completion</title>
    <link href="/2023/11/19/TransR/"/>
    <url>/2023/11/19/TransR/</url>
    
    <content type="html"><![CDATA[<h1id="learning-entity-and-relation-embeddings-for-knowledge-graph-completion">LearningEntity and Relation Embeddings for Knowledge Graph Completion</h1><p><strong>参考博客</strong></p><p>知乎：<ahref="https://yaoleo.github.io/2017/10/27/TransE%E7%AE%97%E6%B3%95%E7%9A%84%E7%90%86%E8%A7%A3/">TransE算法的理解</a></p><p>知乎：<ahref="https://zhuanlan.zhihu.com/p/359925162">TransE、TransH、TransR 和TransD</a></p><p>知乎：<a href="https://zhuanlan.zhihu.com/p/144412694">知识表示学习Trans 系列梳理（论文+代码）</a></p><p> </p><h2 id="abstract">Abstract</h2><p>知识图谱补全=在现有知识图的监督下进行两个实体之间的链路预测</p><p>本文考虑从 knowledge graph embeddings 入手以解决问题。</p><p>前人提出的 TransE 和 TransH等模型，是通过将关系视为从头部实体到尾部实体的转换，来将三元组（实体、属性、关系）翻译成embedding 词向量，即一个<strong>低维的向量</strong>。</p><p>我们注意到这些模型还是把实体和关系放在同一个语义空间中的。</p><p>事实上，一个实体有多个方面，它的每个关系可能聚焦在不同的方面，这使得采用相同的语义空间不足以建模。直观的是，一些实体是相似的，因此在实体空间中彼此接近，但在某些特定方面却大不相同，因此在相应的关系空间中彼此相距很远。</p><p>在本文中，我们提出了 TransR在单独的实体空间和关系空间中构建实体和关系嵌入。</p><p>之后，我们先将实体从实体空间投影到相应的关系空间来学习嵌入，然后在投影的实体之间建立关系。</p><p>在实验中，我们在三个任务上评估我们的模型，包括链接预测、三重分类和关系事实提取。</p><p>源码：https://github.com/mrlyk423/relation_extraction</p><p> </p><h2 id="introduction">Introduction</h2><blockquote><p>由于对链路预测不熟，所以看了 introduction。</p></blockquote><p>三元组 (h, r, t)，符号含义：</p><ul><li>h：head entity</li><li>r：relation</li><li>t：tail entity</li></ul><p>实体空间中的 h 和 t 被投影到关系空间后，被表示为 h_r 和t_r，三者之间的关系是 h_r + r ≈ t_r。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/TransR1.png"alt="TransR的基本思想" /><figcaption aria-hidden="true">TransR的基本思想</figcaption></figure><p>这种 relation-specific 的投影可以使之间存在关系的头部实体 h_r和尾部实体 t_r（圆形）彼此接近，同时还能远离与它们没有关系的实体（三角形）。</p><p>在 head-tail实体对中，实体之间关系的模式非常多样。例如，“location-location-contains”就可以是“country-city”、“country-university”或者“continent-country”。我们通过将不同的head-tail 实体对聚类成组，并为每个组学习不同的关系向量来扩展TransR，称为基于聚类的 TransR（CTransR）。</p><p> </p><h2 id="related-models">Related Models</h2><h3 id="transe">TransE</h3><p>TransE 认为一个正确的三元组的 embedding 向量 (h, r, t)会满足公式：h+r=t，即头实体的 embedding 加上关系的 embedding会等于尾实体的 embedding。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/TransE2.jpg"alt="TransE的基本思想" /><figcaption aria-hidden="true">TransE的基本思想</figcaption></figure><p>如果是一个错误的三元组，那么三者的 embedding 就不会满足这种关系。</p><p>根据上述观点，TransE 的 score function 如下图所示：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/TransE.png"alt="TransE的score function及其缺点" /><figcaption aria-hidden="true">TransE的scorefunction及其缺点</figcaption></figure><p>虽然 TransE 具有训练速度快、易于实现等优点，但是<strong>它不能够处理N-to-1、1-to-N 和 N-to-N 的关系</strong>。以 N-to-1 关系为例，三元组(h_i, r, t) 中的 r 和 t 固定，TransE 为了满足三角闭包关系，训练出来的h_i 向量会很相似。</p><p>同理，处理 1-to-N关系时也会存在此类问题。例如，知识库中有两个三元组，分别是 (美国, 总统,特朗普) 和 (美国, 总统, 拜登)，这里的“总统”关系是典型的 1-to-N关系。如果用 TransE从这两个三元组学习知识表示，则会使“特朗普”和“拜登”的向量变得相同，这显然不符合事实。</p><p> </p><h4 id="l2-范数">L2 范数</h4><p>L2 范数，又称为欧几里得范数，可以防止过拟合，提升模型的泛化能力。</p><p>其计算步骤如下：</p><ol type="1"><li>对向量的每个维度进行平方操作</li><li>将所有平方值相加</li><li>对结果取平方根</li></ol><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/L2.png"alt="L2的计算公式" /><figcaption aria-hidden="true">L2的计算公式</figcaption></figure><p> </p><h4 id="范数">范数</h4><p>范数（Norm）是一个对向量空间中的向量进行度量或测量的概念，可以理解为<strong>向量的大小或长度</strong>。</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/Lp.png"alt="范数的定义" /><figcaption aria-hidden="true">范数的定义</figcaption></figure><p>其中 p 是一个正整数或是无穷大。常见的范数包括：</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/Lp2.png"alt="常见的范数" /><figcaption aria-hidden="true">常见的范数</figcaption></figure><blockquote><p>此外，L0 范数是指向量中非 0 的元素的个数。</p><p>来自知乎：https://zhuanlan.zhihu.com/p/28023308</p></blockquote><p> </p><h3 id="transh">TransH</h3><p><strong>英文全称：</strong>translating on hyperplanes（超平面）</p><p><strong>核心思想：</strong>对于每一个关系relation，都由一个超平面上的关系向量 r 和该超平面的法向量（the normalvector）W_r 表示，而不是和实体、关系在同一个嵌入空间。</p><p>具体来说，对于每个三元组 (h, r, t)，将头实体 h 和尾实体 t都映射到超平面 W_r 上，得到向量 h⊥ 和 t⊥，训练使h⊥+r≈t⊥。目的是通过不同的关系拥有的不同的法平面，让同一个实体 h 或 t在不同关系中拥有不同的意义。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/TransH2.png"alt="TransH和TransE的对比" /><figcaption aria-hidden="true">TransH和TransE的对比</figcaption></figure><p>回到我们上面的 N-to-1 的复杂关系问题。对于三元组 (h_1, r, t) 和 (h_2,r, t)，根据 TransE 的思想，则有 h_1=h_2；而根据 TransH 的思想，只需满足h_1 和 h_2 在关系 r 的超平面 W_r 上的投影相同即可。这样就可以区分出 h_1和 h_2，因为两个向量的向量表示是不同的。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/TransH.png"alt="TransH的score function" /><figcaption aria-hidden="true">TransH的score function</figcaption></figure><p> </p><h4 id="向量投影">向量投影</h4><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/proj.png"alt="向量投影公式" /><figcaption aria-hidden="true">向量投影公式</figcaption></figure><blockquote><p>TransH 限制了 W_r 的模长为 1，所以公式长那样。</p></blockquote><p> </p><h4 id="超平面">超平面</h4><p>超平面（hyperplane）是一个比所嵌套的空间低一维的线性子空间。具体来说，在二维空间中，超平面是一条直线；在三维空间中，它是一个平面；在更高维度的空间中，它是一个超曲面。</p><p> </p><h2 id="our-method">Our Method</h2><h3 id="transr">TransR</h3><p>TransR 在不同的语义空间表示实体和关系，并用 relation-specific的矩阵将两个空间连接起来。效果提升不大，但计算量显著增大。</p><p>创新点是将 TransH的“投影到超平面”更进一步“投影到空间”，本质是将投影向量换为投影矩阵。实体还是用一个向量表示，关系用一个向量r 和一个矩阵 M_r 表示。变量定义如下：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/TransR2.png"alt="变量定义" /><figcaption aria-hidden="true">变量定义</figcaption></figure><p>实体空间（h 和 t）的维度和关系空间（r）的维度不同，由矩阵 M_r完成从实体空间到关系空间的投影。TransR 的 score function 如下：</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/TransR3.png"alt="score function" /><figcaption aria-hidden="true">score function</figcaption></figure><p>score function 的模式和 TransE、TransH的相同，此外还增加了一些约束。</p><p> </p><h3 id="cluster-based-transr-ctransr">Cluster-based TransR(CTransR)</h3><p>不管是 TransE、TransH 还是 TransR，为每个 relation 都只学习了一个矩阵M，这意味着它们不能拟合到该关系下的所有实体对，因为关系的模式相当多样。</p><p>基本思想：</p><p>首先将输入实例分成几个组。对于特定的关系 r，将它对应的所有实体对 (h,t) 聚类成多个组。对于每个组里的实体对，它们都应该表现出相同的 r关系。所有实体对 (h, t) 都用它们的向量偏移量 (h − t) 来进行聚类，其中 h和 t 是使用 TransE 获得。然后，我们分别为每个聚类学习关系向量r_c，为每个关系学习矩阵 M_r。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/CTransR.png"alt="CTransR的score function" /><figcaption aria-hidden="true">CTransR的score function</figcaption></figure><p>r_c-r 保证 cluster-specific 的关系向量 r_c 不要离原始的关系向量 r太远，CTransR 也对 h、r、t 和 M 的范数有约束。</p><p> </p><h3 id="training-method-and-implementation-details">Training Method andImplementation Details</h3><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/loss.png"alt="损失函数" /><figcaption aria-hidden="true">损失函数</figcaption></figure><p>S 是正确的三元组（正例）的集合，S' 是错误的三元组（负例）的集合。</p><p><strong>负例的产生</strong></p><p>通常我们得到的知识库是三元组的集合，所有在知识库中出现了的三元组都会被看作是正例。我们通常使用替换法来获取负例，即用其它实体替换掉正例中的实体。对于三元组(h, r, t)，我们会随机抽取知识库中的某个实体 h' 来替换 h，或者用某个实体t' 来替换 t，这样我们就得到了两个负例 (h', r, t) 和 (h, r, t')。</p><p>对于那些 1-to-N、N-to-1 和 N-to-N关系，通过提供更多替换“one”边的机会，可以减少生成false-negative（假阴性）实例的机会。</p><p><strong>反向传播和初始化</strong></p><p>反向传播用的是随机梯度下降法（stochastic gradient descent,SGD）；为了避免过拟合，使用 TransE的结果来初始化实体和关系的向量，使用单位矩阵来初始化关系矩阵。</p>]]></content>
    
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>知识图谱构建流程详解</title>
    <link href="/2023/11/17/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B/"/>
    <url>/2023/11/17/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B/</url>
    
    <content type="html"><![CDATA[<p><strong>参考博客</strong></p><p>知乎：https://zhuanlan.zhihu.com/p/145447330</p><p>博客园：https://www.cnblogs.com/huangyc/p/10043749.html</p><p>综述：<ahref="http://kns-cnki-net-s.vpn.uestc.edu.cn:8118/KXReader/Detail?invoice=TfXLC2ZSNT9XLq1SP5F5Q9DYo1dpuUSVTM87GLoVDLiokXx5M8xMFoGMu7aMK%2F3Ct%2Ffki%2FLdQgDhVvrV505vCEn2VAA3pa6x0jPIWd5WW%2FalFPDMCs3HfKHXXce3tMJ5b1t90Tg%2BSvdD9M68sD10nMMgtW8puh%2BwjUu1P858iwA%3D&amp;DBCODE=CJFQ&amp;FileName=JFYZ201603009&amp;TABLEName=cjfdlast2016&amp;nonce=49E73AC3869B4489BBF4BDE9628A1CDA&amp;TIMESTAMP=1700221256698&amp;uid=">知识图谱构建技术综述</a></p><p>清华 XLore：https://xlore.cn/index</p><p> </p><h2 id="一知识图谱简介">一、知识图谱简介</h2><h3 id="历史沿革">1.1 历史沿革</h3><p>提出知识图谱的目的：</p><ul><li>实现智能化语义检索</li><li>解决信息检索问题</li></ul><p>虽然知识图谱的概念较新，但它并非是一个全新的研究领域。早在 2006年，Berners-Lee就提出了数据链接（linkeddata）的思想，呼吁推广和完善相关的技术标准如：</p><ul><li>URI（uniform resource identifier）</li><li>RDF（resource description framework）</li><li>OWL（ontology language）</li></ul><p>为迎接<strong>语义网络</strong>时代的到来做好准备。随后掀起了一场语义网络研究热潮，知识图谱技术正是建立在相关的研究成果之上的，是对现有语义网络技术的一次扬弃和升华。</p><p> </p><h3 id="定义">1.2 定义</h3><p>定义：知识图谱是<strong>结构化的语义知识库</strong>，用于以符号形式描述物理世界中的概念及其相互关系。其基本组成单位是<strong>“实体-关系-实体"三元组，以及实体及其相关属性-值对</strong>。实体间通过关系相互联结，构成网状的知识结构。</p><p>通过知识图谱，可以实现 web从网页链接（web1.0）向概念链接转变，<strong>支持用户按主题而不是字符串检索</strong>，从而真正实现语义检索。基于知识图谱的搜索引擎，能够<strong>以图形方式向用户反馈结构化的知识</strong>，用户不浏览大量网页，就可以准确定位和深度获取知识。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/graphpng.png"alt="诸葛亮-清华博拉图" /><figcaption aria-hidden="true">诸葛亮-清华博拉图</figcaption></figure><p>上述定义包含 3 层含义：</p><ul><li>知识图谱是由许多具有属性的实体通过关系链接而成的网状知识库。从图的角度来看，知识图谱在本质上是一种<strong>概念网络</strong>，其中的<strong>节点表示物理世界的实体（或概念）</strong>，而实体间的各种语义关系则构成网络中的边。由此，知识图谱是对物理世界的一种符号表达。</li><li>知识图谱的研究价值在于，它是构建在当前 web基础之上的一层<strong>覆盖网络（overlaynetwork）</strong>，借助知识图谱，能够在 web网页之上建立概念间的链接关系，从而以最小的代价将互联网中积累的信息组织起来，成为可以被利用的知识<strong>（信息--&gt;知识）</strong></li><li>知识图谱的应用价值在于，它能够改变现有的信息检索方式，一方面<strong>通过推理实现概念检索</strong>（相对于现有的字符串模糊匹配方式而言）；另一方面以<strong>图形化方式</strong>向用户展示经过分类整理的结构化知识，从而使人们从人工过滤网页寻找答案的模式中解脱出来。</li></ul><p>知识图谱的分类：</p><ul><li>用于构建结构化的百科知识的“通用知识图谱”</li><li>基于行业数据构建和应用的“领域知识图谱”</li></ul><p> </p><h3 id="架构">1.3 架构</h3><p>知识图谱的架构包含：</p><ul><li>知识图谱自身的逻辑结构，分为数据层、模式层</li><li>构建知识图谱所采用的技术架构</li></ul><p><strong>知识图谱自身的逻辑结构</strong></p><p>在知识图谱的数据层，<strong>知识以事实（fact）为单位存储在图数据库</strong>。例如谷歌的Graphd 和微软的 Trinity都是典型的图数据库。如果<strong>以“实体-关系-实体”或者“实体-属性-属性值”三元组作为事实的基本表达方式</strong>，则存储在图数据库中的所有数据将构成庞大的实体关系网络，形成知识的“图谱”。</p><p>模式层位居数据层之上，是知识图谱的核心。在模式层存储的是经过提炼的知识，通常采用<strong>本体库</strong>来管理知识图谱的模式层，借助<strong>本体库对公理、规则和约束条件的支持能力</strong>来规范实体、关系以及实体的类型和属性等对象之间的联系。本体库在知识图谱中的地位相当于知识库的模具，拥有本体库的知识库冗余知识较少。</p><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/human.png"alt="本体定义-人类" /><figcaption aria-hidden="true">本体定义-人类</figcaption></figure><p><strong>构建知识图谱所采用的技术架构</strong></p><p>下图给出了知识图谱技术的整体架构，其中虚线框内的部分为知识图谱的构建过程，同时也是知识图谱更新的过程。</p><figure><imgsrc="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/process.jpg"alt="知识图谱构建过程" /><figcaption aria-hidden="true">知识图谱构建过程</figcaption></figure><p>如图所示，<strong>知识图谱的构建过程</strong>是从原始数据出发，采用一系列自动或半自动的技术手段，从原始数据中提取出知识要素（即事实），并将其存入知识库的数据层和模式层的过程。</p><p>这是一个迭代更新的过程，根据知识获取的逻辑，每一轮迭代包含 3个阶段：</p><ol type="1"><li><strong>信息抽取：</strong>从各种类型的数据源中提取出实体、属性以及实体间的相互关系，在此基础上将知识以本体的形式进行表达（本体对实体、属性和关系制定了规则和约束）；</li><li><strong>知识融合：</strong>在获得新知识之后，还需要对其进行整合，以消除矛盾和歧义。比如某些实体可能有多种表达，某个特定称谓也许对应于多个不同的实体等；</li><li><strong>知识加工：</strong>对于经过融合的新知识，还需要接受质量评估（部分需要人工参与甄别），合格的部分才能被加入到知识库中，以确保知识库的质量。新增数据之后，可以进行<strong>知识推理</strong>，以拓展现有知识，得到新的知识。</li></ol><p>知识图谱有 2 种构建方式：</p><ul><li><strong>自顶向下构建：</strong>是指借助百科类网站等结构化数据源，从高质量数据中提取本体和模式信息，加入到知识库中。</li><li><strong>自底向上构建：</strong>是指借助一定的技术手段，从公开采集的数据中<strong>提取出资源模式</strong>，选择其中置信度较高的新模式，经人工审核之后，加入到知识库中。如：谷歌的Knowledge Vault、微软的 Satori 等。</li></ul><blockquote><p>GPT：提取出资源模式就是指提取出数据中包含的实体、属性、关系。</p></blockquote><p> </p><p><strong>Data Acquisition 中的数据类型</strong></p><p>知识图谱的<strong>原始数据</strong>类型一般来说有三类，也是互联网上的三类原始数据：</p><ul><li>结构化数据（Structed Data）：如关系数据库</li><li>半结构化数据（Semi-Structed Data）：如 XML、JSON、百科</li><li>非结构化数据（UnStructed Data）：如图片、音频、视频、文本</li></ul><p>原始数据存储方式：</p><ul><li>使用 RDF（资源描述框架）等规范存储格式来进行存储</li><li>使用 Neo4j 等常用的图数据库来进行存储</li></ul><blockquote><p>在知识图谱方面，图数据库比关系数据库灵活的多。在数据少的时候，关系数据库也没有问题，效率也不低。但是随着知识图谱变的复杂，图数据库的优势会明显增加。当涉及到2、3度的关联查询，基于图数据库的效率会比关系数据库的效率高出几千倍甚至几百万倍。</p></blockquote><p> </p><hr /><p>接下来介绍的是“知识图谱的构建技术”。</p><hr /><p> </p><h2 id="二信息抽取">二、信息抽取</h2><p>信息抽取（infromationextraction）是知识图谱构建的第1步，其中的关键问题是：如何从异构数据源中自动抽取信息得到候选知识单元？（候选知识单元：被认为可能包含有用知识的单元或片段。）</p><p>信息抽取是一种自动化地从半结构化、非结构化数据中抽取<strong>实体、关系以及实体属性等结构化信息</strong>的技术。</p><p>关键技术包括：实体抽取、关系抽取、属性抽取。</p><p> </p><h3 id="实体抽取entity-extraction">2.1 实体抽取（EntityExtraction）</h3><p>实体抽取又称为命名实体识别（named entityrecognition，NER），是指<strong>从文本数据集中自动识别出命名实体</strong>。</p><p>实体抽取的质量（准确率和召回率）对后续的知识获取效率和质量影响极大，因此是信息抽取中最为基础和关键的部分。</p><blockquote><p>召回率</p><p>$Recall= $</p><p>用于衡量模型能够正确识别正例样本的能力。</p></blockquote><p>早期对实体抽取方法的研究主要面向单一领域（如特定行业或特定业务），关注如何识别出文本中的人名、地名等专有名词和有意义的时间等实体信息。</p><table><thead><tr class="header"><th>方法</th><th>效果</th></tr></thead><tbody><tr class="odd"><td>Raul；基于规则的方法</td><td>耗费大量的人力，且可扩展性较差，难以适应数据的变化</td></tr><tr class="even"><td>Liu；基于有监督学习；K-最近邻算法和条件随机场模型</td><td>在准确率和召回率上的表现都不够理想，且算法的性能依赖于训练样本的规模</td></tr><tr class="odd"><td>Lin；基于有监督学习和规则；采用字典辅助下的最大熵算法</td><td>在基于 Meline 论文摘要的 GENIA数据集上取得了实体抽取准确率和召回率均超过 70％ 的实验结果</td></tr></tbody></table><p>随着命名实体识别技术不断取得进展，学术界开始关注开放域（opendomain）的信息抽取问题，即不再限定于特定的知识领域，而是面向开放的互联网，研究和解决全网信息抽取问题。</p><table><thead><tr class="header"><th>方法</th><th>效果</th></tr></thead><tbody><tr class="odd"><td>Sekine；命名实体分类体系</td><td>将网络中的所有命名实体划分为 150 个分类</td></tr><tr class="even"><td>Ling；借鉴 Freebase 的实体分类方法</td><td>归纳出 112个实体类别，并基于条件随机场模型进行实体边界识别，最后采用自适应感知机算法实现了对实体的自动分类，其实验结果显著优于StanfordNER 等当前主流的命名实体识别系统</td></tr></tbody></table><p>然而，互联网中的内容是动态变化的，web2.0技术更进一步推动了互联网的概念创新，采用人工预定义实体分类体系的方式已经很难适应时代的需求。</p><p>面向开放域的实体抽取和分类技术能够较好地解决这一问题，该方法的基本思想是对于任意给定的实体，采用统计机器学习的方法，从目标数据集（通常是网页等文本数据）中抽取出与之具有相似上下文特征的实体，从而实现实体的分类和聚类。在面向开放域的实体识别和分类研究中，不需要（也不可能）为每个领域或每个实体类别建立单独的语料库作为训练集。因此，该领域面临的主要挑战是如何从给定的少量实体实例中自动发现具有区分力的模式。</p><table><thead><tr class="header"><th>方法</th><th>效果</th></tr></thead><tbody><tr class="odd"><td>Whitelaw；提出了一种迭代扩展实体语料库的解决方案</td><td>基本思路是根据已知的实体实例进行特征建模，利用该模型处理海量数据集得到新的命名实体列表，然后针对新实体建模，迭代地生成实体标注语料库</td></tr><tr class="even"><td>Jain；提出了一种面向开放域的无监督学习算法；通过搜索引擎的服务器日志获取新出现的命名实体</td><td>事先并不给出实体分类，而是基于实体的语义特征从搜索日志中识别出命名实体，然后采用聚类算法对识别出的实体对象进行聚类，该方法已经在搜索引擎技术中得到应用，用于根据用户输入的关键字自动补全信息</td></tr></tbody></table><p> </p><h3 id="关系抽取relation-extraction">2.2 关系抽取（RelationExtraction）</h3><p>文本语料经过实体抽取，得到的是一系列离散的命名实体，为了得到语义信息，还需要<strong>从相关的语料中提取出实体之间的关联关系</strong>，通过关联关系将实体（概念）联系起来，才能够形成网状的知识结构。研究关系抽取技术的目的，就是解决如何从文本语料中抽取实体间的关系这一基本问题。</p><ol type="1"><li>人工构造语法和语义规则（模式匹配）</li><li>统计机器学习方法</li><li>基于特征向量或核函数的有监督学习方法</li><li>研究重点转向半监督和无监督</li><li>开始研究面向开放域的信息抽取方法</li><li>将面向开放域的信息抽取方法和面向封闭领域的传统方法结合</li></ol><p> </p><h3 id="属性抽取attribute-extraction">2.3 属性抽取（AttributeExtraction）</h3><p>属性抽取的目标是从不同信息源中采集特定实体的属性信息。例如针对某个公众人物，可以从网络公开信息中得到其昵称、生日、国籍、教育背景等信息。属性抽取技术能够从多种数据来源中汇集这些信息，实现对实体属性的完整勾画。</p><p>由于可以将实体的属性视作实体与属性值之间的一种名词性关系，因此也可以<strong>将属性抽取任务转化为关系抽取任务</strong>。</p><ul><li>基于规则和启发式算法，抽取结构化数据。</li><li>基于百科类网站的半结构化数据，通过自动抽取生成训练语料，用于训练实体属性标注模型，然后将其应用于对非结构化数据的实体属性抽取。</li><li>采用数据挖掘的方法直接从文本中挖掘实体属性和属性值之间的关系模式，据此实现对属性名和属性值在文本中的定位。</li></ul><p>采用数据挖掘的方法的基本假设是属性名和属性值之间有位置上的关联关系，事实上在真实语言环境中，许多实体属性值附近都存在一些用于限制和界定该属性值含义的<strong>关键字（属性名）</strong>，在自然语言处理技术中将这类属性称为<strong>有名属性</strong>，因此<strong>可以利用这些关键字来定位有名属性的属性值</strong>。</p><p> </p><p> </p><h2 id="三知识融合">三、知识融合</h2><p>通过信息抽取，我们就从原始的半结构化、非结构化数据中获得了实体、关系以及实体的属性信息。</p><p>仍需解决的问题：</p><ul><li>信息之间的关系是扁平化的，缺乏层次性和逻辑性</li><li>知识中还存在大量冗杂和错误的信息</li></ul><p>知识融合包括两部分内容：实体链接、知识合并。</p><p> </p><h3 id="实体链接entity-linking">3.1 实体链接（Entity Linking）</h3><p>实体链接是指将文本中抽取得到的实体对象链接到知识库中对应的实体对象。</p><p>基本思想：</p><ol type="1"><li>根据给定的<strong>实体指称项</strong>，从知识库中选出一组候选实体对象；</li><li>通过相似度计算将实体指称项链接到正确的实体对象。</li></ol><p>研究历史：</p><ol type="1"><li>仅关注如何将从文本中抽取到的实体链接到知识库中，忽视了位于同一文档的实体间存在的语义联系。</li><li>开始关注利用实体的共现关系，同时将多个实体链接到知识库中，即集成实体链接（collectiveentity linking）。</li></ol><p>实体链接的流程：</p><ul><li>从文本中通过实体抽取得到实体指称项。</li><li>进行<strong>实体消歧</strong>和<strong>共指消解</strong>：<ul><li>判断知识库中同名的实体是否与之表示不同的含义</li><li>判断知识库中不同名的实体是否与之表示相同的含义</li></ul></li><li>在确认知识库中对应的正确实体对象之后，将该实体指称项链接到知识库中对应实体。</li></ul><blockquote><p><strong>实体指称项</strong></p><p>在知识图谱中，实体指标项通常是指关联到特定实体的属性或特征，这些属性或特征可以用来描述该实体。实体指标项有助于更全面地了解和表达实体的性质、特征或其它相关信息。以下是一些可能包含在知识图谱中的实体指标项的示例：</p><ul><li><p>人物实体的指标项：</p><ul><li>姓名</li><li>出生日期</li><li>出生地</li><li>职业</li><li>教育背景</li><li>家庭关系</li></ul></li><li><p>地点实体的指标项：</p><ul><li>名称</li><li>地理坐标</li><li>人口</li><li>面积</li><li>历史事件</li><li>相关地理特征</li></ul></li></ul></blockquote><p> </p><h4 id="实体消歧">3.1.1 实体消歧</h4><p>实体消歧主要用于解决同名的实体之间存在歧义的问题。</p><p>例如，“李娜”这个名词（指称项）可以对应于作为歌手的李娜这个实体，也可以对应于作为网球运动员的李娜这个实体。</p><p>通过实体消歧，就可以根据当前的语境，准确建立实体链接，实体消歧主要采用聚类法。聚类法是指以实体对象为聚类中心，将所有指向同一目标实体对象的指称项聚集到以该对象为中心的类别下。采用聚类法消歧的关键问题是<strong>如何定义实体对象与指称项之间的相似度</strong>，常用方法有4 种。</p><p> </p><h4 id="共指消解">3.1.2 共指消解</h4><p>共指消解主要用于解决多个实体指称项对应同一实体对象的问题。</p><p>例如，在一篇新闻稿中，“Barack Obama”、“president Obama”、“thepresident” 等指称项指向的是同一实体对象，其中的许多代词如 “he”、“him”等，也可能指向该实体对象。</p><p>利用共指消解技术，可以<strong>将这些指称项关联（合并）到正确的实体对象</strong>，由于该问题在信息检索和自然语言处理等领域具有特殊的重要性，吸引了大量的研究努力。</p><p>共指消解还有一些其它的名字，比如：</p><ul><li>对象对齐（object alignment）</li><li>实体匹配（entity matching）</li><li>实体同义（entity synonyms）</li></ul><p> </p><h3 id="知识合并">3.2 知识合并</h3><p>在构建知识图谱时，可以从第三方知识库产品或已有的结构化数据获取知识输入。</p><p>常见的知识合并需求有两个：</p><ul><li>合并外部知识库</li><li>合并关系数据库</li></ul><p>合并外部知识库：</p><ul><li>数据层的融合，包括实体的指称、属性、关系以及所属类别等，主要的问题是如何避免实例以及关系的冲突问题，造成不必要的冗余</li><li>模式层的融合，将新得到的本体融入已有的本体库中</li></ul><p>合并关系数据库：在知识图谱构建过程中，一个重要的高质量知识来源是企业或者机构自己的关系数据库。为了将这些结构化的历史数据融入到知识图谱中，可以采用资源描述框架（RDF）作为数据模型。业界和学术界将这一数据转换过程形象地称为RDB2RDF，其实质就是将关系数据库的数据换成 RDF 的三元组数据。</p><p> </p><p> </p><h2 id="四知识加工">四、知识加工</h2><p>在前面，我们已经通过信息抽取，从原始语料中提取出了实体、关系与属性等知识要素，并且经过知识融合，消除实体指称项与实体对象之间的歧义，得到一系列基本的事实表达。</p><p>然而事实本身并不等于知识。要想最终获得结构化，网络化的知识体系，还需要经历知识加工的过程。</p><p>知识加工主要包括 3 方面内容：本体构建、知识推理、质量评估。</p><p> </p><h3 id="本体构建">4.1 本体构建</h3><p>本体（ontology）是指人工的概念集合、概念框架，是描述客观世界的抽象模型，如“人”、“事”、“物”等。在知识图谱中，本体位于模式层，用于描述概念层次体系，是知识库中知识的概念模板。</p><p>本体的最大特点在于它是<strong>共享的</strong>：</p><ul><li>本体中反映的知识是一种明确定义的共识</li><li>本体是同一领域内不同主体间交流的语义基础</li></ul><p>本体是树状结构，相邻层次的节点（概念）之间具有严格的“IsA”关系，这种单纯的关系<strong>有助于知识推理，但却不利于表达概念的多样性</strong>。</p><p> </p><p>本体可以采用人工编辑的方式手动构建，即借助<strong>本体编辑软件</strong>：</p><figure><img src="https://me.jinchuang.org/usr/uploads/2020/01/2122950147.png"alt="本体编辑软件-protege" /><figcaption aria-hidden="true">本体编辑软件-protege</figcaption></figure><p>也可以用<strong>数据驱动的自动化方式</strong>构建本体，包含三个阶段：实体并列关系相似度计算、实体上下位关系抽取、本体的生成。</p><ol type="1"><li><strong>实体并列关系相似度：</strong>用于考察任意给定的 2个实体在多大程度上属于同一概念分类的指标测度，相似度越高，表明这 2个实体越有可能属于同一语义类别。所谓并列关系，是相对于纵向的概念隶属关系而言的。例如“中国”和“美国”作为国家名称的实体，具有较高的并列关系相似度；而“美国”和“手机”这2个实体，属于同一语义类别的可能性较低，因此具有较低的并列关系相似度。</li><li><strong>实体上下位关系抽取：</strong>用于确定概念之间的隶属（IsA）关系，这种关系也称为上下位关系，例如，词组（导弹，武器）构成上下位关系，其中的“导弹”为下位词，“武器”为上位词。</li><li><strong>本体生成：</strong>用于对得到的概念进行聚类，并对其进行语义类的标定，即为该类中的实体指定1 个或多个公共上位词。</li></ol><p><strong>举例说明</strong></p><ol type="1"><li><strong>计算实体并列关系相似度：</strong>当知识图谱刚刚得到“中国”、“美国”、“手机”这三个实体（概念）时，可能会认为它们三个之间并没有什么差别。但当它去计算三个实体之间的相似度后，就会发现“中国”、“美国”之间相似度高，可能属于同一语义类别；“中国”和“手机”、“美国”和“手机”之间相似度低，可能不属于同一语义类别。</li><li><strong>抽取实体上下位关系：</strong>确定“中国”和“美国”这两个实体共同的上位概念。在这个例子中，可能的上位概念是“国家”或“主权国家”。确定上位概念后，查找表达实体上下位关系的词语。常见的上下位关系词语包括：“是一种”、“属于”、“包括”等，可能会用到类似“中国是一个国家”、“美国是一个国家”的表达方式。</li><li><strong>生成本体：</strong>在这个例子中，我们将“中国”和“美国”聚类到“国家”类别中，而“手机”则属于“科技产品”类别。然后为每个类别中的实体指定了一个或多个公共上位词。“中国”和“美国”可以共享“政治实体”作为它们的公共上位词，而“手机”可以有“智能设备”作为其公共上位词。</li></ol><p> </p><h3 id="知识推理">4.2 知识推理</h3><blockquote><p>在我们完成了本体构建这一步之后，一个知识图谱的雏形便已经搭建好了。但可能在这个时候，知识图谱之间大多数关系都是残缺的，缺失值非常严重，那么这个时候，我们就可以使用知识推理技术，去完成进一步的知识发现。</p></blockquote><p>知识推理是指从知识库中已有的实体关系数据出发，经过计算机推理，建立实体间的新关联，从而拓展和丰富知识网络。</p><p>例如，已知（乾隆，父亲，雍正）和（雍正，父亲，康熙），可以得到（乾隆，祖父，康熙）或（康熙，孙子，乾隆）。知识推理的对象并不局限于实体间的关系，也可以是实体的属性值、本体的概念层次关系等。例如，已知某实体的生日属性，可以通过推理得到该实体的年龄属性。根据本体库中的概念继承关系，也可以进行概念推理。例如，已知（老虎，科，猫科）和（猫科，目，食肉目），可以推出（老虎，目，食肉目）。</p><p>这一块的算法主要可以分为 3大类，<strong>基于逻辑的推理、基于图的推理、基于深度学习的推理</strong>。</p><p> </p><h3 id="质量评估">4.3 质量评估</h3><p>质量评估也是知识库构建技术的重要组成部分，这一部分存在的意义在于：可以对知识的可信度进行量化，通过舍弃置信度较低的知识来保障知识库的质量。</p><p> </p><p> </p><h2 id="五知识更新">五、知识更新</h2><p>从逻辑上看，知识库的更新包括数据层的更新和概念层的更新。<strong>数据层的更新</strong>主要是新增或更新实体、关系和属性值，对数据层进行更新时需要考虑数据源的可靠性、数据的一致性（是否存在矛盾或冗余等问题）等多方面因素。<strong>概念层的更新</strong>是指新增数据后获得了新的概念，需要自动将新的概念添加到知识库的概念层中。</p><p>知识图谱的内容更新有两种方式：</p><ul><li>全面更新：指以更新后的全部数据为输入，从零开始构建知识图谱。这种方法比较简单，但资源消耗大，而且需要耗费大量人力资源进行系统维护；</li><li>增量更新：以当前新增数据为输入，向现有知识图谱中添加新增知识。这种方式资源消耗小，但目前仍需要大量人工干预（定义规则等），因此实施起来十分困难。</li></ul><p> </p><p> </p><h2 id="六总结">六、总结</h2><p>通过知识图谱技术，不仅可以将互联网的信息表达成更接近人类认知世界的形式，而且提供了一种更好的组织、管理和利用海量信息的方式。目前的知识图谱技术主要用于<strong>智能语义搜索、移动个人助理（Siri）以及深度问答系统（Watson）</strong>，支撑这些应用的核心技术正是知识图谱技术。</p><p>在智能语义搜索中，当用户发起查询时，搜索引擎会借助知识图谱的帮助对用户查询的关键词进行解析和推理，进而将其映射到知识图谱中的一个或一组概念之上，然后根据知识图谱的概念层次结构，向用户返回图形化的知识结构，这就是我们在谷歌和百度的搜索结果中看到的<strong>知识卡片</strong>。</p><p>在深度问答应用中，系统同样会首先在知识图谱的帮助下对用户使用自然语言提出的问题进行语义分析和语法分析，进而将其转化成<strong>结构化形式的查询语句</strong>，然后在知识图谱中查询答案。比如，如果用户提问：『如何判断是否感染了埃博拉病毒？』，则该查询有可能被等价变换为『埃博拉病毒的症状有哪些？』，然后再进行推理变换，<strong>最终形成等价的三元组查询语句</strong>，如（埃博拉，症状，？）和（埃博拉，征兆，？）等。如果由于知识库不完善而无法通过推理来解答用户的问题，深度问答系统还可以利用搜索引擎向用户反馈搜索结果，同时根据搜索结果更新知识库，从而为回答后续的提问提前做出准备。</p>]]></content>
    
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>知识图谱</title>
    <link href="/2023/11/16/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    <url>/2023/11/16/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/</url>
    
    <content type="html"><![CDATA[<h1 id="知识图谱">知识图谱</h1><h2 id="一知识图谱定义">一、知识图谱定义</h2><p>中文：知识图谱</p><p>英文：Knowledge Graph</p><p>功能：以符号形式描述物理世界中的概念及其相互关系。</p><p>基本组成单位：三元组</p><ul><li>实体-关系-实体</li><li>实体-属性-属性值</li></ul><p>实体间通过关系相互联结，构成网状的知识结构。</p><p>知识图谱可以实现 Web从网页链接向概念链接的转变，支持用户按主题而不是字符串检索，真正实现<strong>语义检索。</strong>基于知识图谱的搜索引擎，能够以图形方式向用户反馈结构化的知识，用户不必浏览大量网页即能准确定位和深度获取知识。</p><p> </p><p> </p><h2 id="二语义网技术栈">二、语义网技术栈</h2><figure><img src="https://hexo-0711.oss-cn-chengdu.aliyuncs.com/image/2.png"alt="语义网技术栈" /><figcaption aria-hidden="true">语义网技术栈</figcaption></figure><p> </p><p> </p><h3 id="uri">2.1 URI</h3><p>URI（Uniform ResourceIdentifier，统一资源标识符）是一种用于唯一标识和定位资源的字符序列，包括两个具体的子集：</p><ul><li>URL（Uniform Resource Locator，统一资源定位符）</li><li>URN（Uniform Resource Name，统一资源名称）</li></ul><p> </p><h4 id="url">2.1.1 URL</h4><p>URL 不仅能唯一标识资源，还能定位和获取资源。</p><p>通常，URL 包括协议（如 HTTP 或HTTPS）、主机名、路径和可选的查询参数和片段标识符。</p><p>例如，https://www.ive.com/index.html 是一个 URL。</p><table><thead><tr class="header"><th>协议</th><th>主机名</th><th>路径</th></tr></thead><tbody><tr class="odd"><td>https: 或者 http:</td><td>www.ive.com</td><td>index.html</td></tr></tbody></table><p> </p><h4 id="标识符urn">2.1.2 标识符：URN</h4><p>URN 只能唯一标识资源，不能定位和获取资源。</p><p>设计 URN是为了提供一种持久的标识，即使资源在不同的位置或网络状态下也能被唯一识别。</p><p>通常，URN包括协议标识符（urn）、特定命名空间标识符和特定资源标识符（resource-id）。</p><p>例如，urn:isbn:0451450523 是一个 URN，用于标识一本书的 ISBN 号。</p><table><thead><tr class="header"><th>协议标识符</th><th>特定命名空间标识符</th><th>特定资源标识符</th></tr></thead><tbody><tr class="odd"><td>urn</td><td>isbn</td><td>0451450523</td></tr></tbody></table><p> </p><h4 id="url-和-urn">2.1.3 URL 和 URN</h4><blockquote><p>Q：既然 URL 都这么强了，为什么还要 URN？</p></blockquote><p>URN 弥补了 URL 在资源标识方面的局限性：</p><ol type="1"><li>持久性标识：URN提供的资源标识是持久的，即使资源的位置或访问方式发生变化也不会影响其标识。这意味着即使资源的URL 发生变化，使用 URN 标识的资源仍然可以被准确地引用和访问。</li><li>解耦资源与位置：URN 将资源标识和资源的位置进行解耦。URN仅关注资源的标识和身份，而不依赖于资源的特定位置或获取方式。这对于需要在多个场景中共享和引用资源，而不受资源位置的限制的情况（访问受限、位置变化、失效链接、内容更改）特别有用。</li><li>语义化的标识：URN可以使用具有语义意义的命名空间和标识符来描述资源，使得资源标识更加易读、易理解和易于管理。URN的语义化标识有助于提高资源的可发现性和语义互操作性。</li><li>多个 URL 的映射：一个资源可能具有多个URL，用于不同的访问方式、不同的协议或不同的位置。使用 URN 可以将这些多个URL 映射到一个统一的标识，使得在引用资源时更加方便和一致。</li></ol><p> </p><p> </p><h3 id="语法xml">2.2 语法：XML</h3><p>菜鸟教程：https://www.runoob.com/xml/xml-tutorial.html</p><p>XML 指可扩展标记语言（e<strong>X</strong>tensible<strong>M</strong>arkup <strong>L</strong>anguage）。</p><p><strong>XML被设计用来传输和存储数据，</strong>不用于表现和展示数据，HTML则用来表现数据。</p><p> </p><h4 id="xml-实例">2.2.1 XML 实例</h4><p><img src="知识图谱.assets/3.png" /></p><p>用 XML 来表示上图所示的一本书：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">bookstore</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">book</span> <span class="hljs-attr">category</span>=<span class="hljs-string">&quot;COOKING&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">title</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">&quot;en&quot;</span>&gt;</span>Everyday Italian<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span>Giada De Laurentiis<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">year</span>&gt;</span>2005<span class="hljs-tag">&lt;/<span class="hljs-name">year</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">price</span>&gt;</span>30.00<span class="hljs-tag">&lt;/<span class="hljs-name">price</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">book</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">book</span> <span class="hljs-attr">category</span>=<span class="hljs-string">&quot;CHILDREN&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">title</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">&quot;en&quot;</span>&gt;</span>Harry Potter<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span>J K. Rowling<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">year</span>&gt;</span>2005<span class="hljs-tag">&lt;/<span class="hljs-name">year</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">price</span>&gt;</span>29.99<span class="hljs-tag">&lt;/<span class="hljs-name">price</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">book</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">book</span> <span class="hljs-attr">category</span>=<span class="hljs-string">&quot;WEB&quot;</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">title</span> <span class="hljs-attr">lang</span>=<span class="hljs-string">&quot;en&quot;</span>&gt;</span>Learning XML<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">author</span>&gt;</span>Erik T. Ray<span class="hljs-tag">&lt;/<span class="hljs-name">author</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">year</span>&gt;</span>2003<span class="hljs-tag">&lt;/<span class="hljs-name">year</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">price</span>&gt;</span>39.95<span class="hljs-tag">&lt;/<span class="hljs-name">price</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">book</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">bookstore</span>&gt;</span><br></code></pre></td></tr></table></figure><p> </p><p> </p><h3 id="数据交换rdf">2.3 数据交换：RDF</h3><p>全称：Resource Description Framework</p><p>中文：资源描述框架</p><p>RDF 使用 Web 标识符来标识事物，并通过属性和属性值来描述资源。</p><p>对资源、属性和属性值的解释：</p><ul><li>资源是可拥有 URI 的任何事物，比如 "<ahref="https://www.runoob.com/rdf">https://www.runoob.com//rdf</a>"</li><li>属性是拥有名称的资源，比如 "author" 或 "homepage"</li><li>属性值是某个属性的值，比如 "David" 或 "https://www.runoob.com/"</li></ul><blockquote><p>注意：一个属性值除了可以是字面值，还可以是另外一个资源。</p></blockquote><p> </p><h4 id="rdf-陈述">2.3.1 RDF 陈述</h4><p>资源、属性和属性值的组合可形成一个陈述（被称为陈述的主体、谓语和客体）。</p><p>请看一些陈述的具体例子，来加深理解：</p><p>陈述："The author of <ahref="https://www.runoob.com/rdf">https://www.runoob.com//rdf</a> isDavid."</p><ul><li>陈述的主体是：https://www.runoob.com//rdf</li><li>谓语是：author</li><li>客体是：David</li></ul><p>陈述："The homepage of <ahref="https://www.runoob.com/rdf">https://www.runoob.com//rdf</a> ishttps://www.runoob.com/".</p><ul><li>陈述的主体是：https://www.runoob.com//rdf</li><li>谓语是：homepage</li><li>客体是：https://www.runoob.com/</li></ul><p> </p><h4 id="rdf-实例">2.3.2 RDF 实例</h4><p>这是一个 CD 列表的其中几行：</p><table><thead><tr class="header"><th><strong>标题</strong></th><th><strong>艺术家</strong></th><th><strong>国家</strong></th><th><strong>公司</strong></th><th><strong>价格</strong></th><th><strong>年份</strong></th></tr></thead><tbody><tr class="odd"><td>Empire Burlesque</td><td>Bob Dylan</td><td>USA</td><td>Columbia</td><td>10.90</td><td>1985</td></tr><tr class="even"><td>Hide your heart</td><td>Bonnie Tyler</td><td>UK</td><td>CBS Records</td><td>9.90</td><td>1988</td></tr></tbody></table><p>这是一个 RDF 文档的其中几行：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">rdf:RDF</span></span><br><span class="hljs-tag"><span class="hljs-attr">xmlns:rdf</span>=<span class="hljs-string">&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;</span></span><br><span class="hljs-tag"><span class="hljs-attr">xmlns:cd</span>=<span class="hljs-string">&quot;http://www.recshop.fake/cd#&quot;</span>&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">rdf:Description</span></span><br><span class="hljs-tag"><span class="hljs-attr">rdf:about</span>=<span class="hljs-string">&quot;http://www.recshop.fake/cd/Empire Burlesque&quot;</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:artist</span>&gt;</span>Bob Dylan<span class="hljs-tag">&lt;/<span class="hljs-name">cd:artist</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:country</span>&gt;</span>USA<span class="hljs-tag">&lt;/<span class="hljs-name">cd:country</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:company</span>&gt;</span>Columbia<span class="hljs-tag">&lt;/<span class="hljs-name">cd:company</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:price</span>&gt;</span>10.90<span class="hljs-tag">&lt;/<span class="hljs-name">cd:price</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:year</span>&gt;</span>1985<span class="hljs-tag">&lt;/<span class="hljs-name">cd:year</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">rdf:Description</span>&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">rdf:Description</span></span><br><span class="hljs-tag"><span class="hljs-attr">rdf:about</span>=<span class="hljs-string">&quot;http://www.recshop.fake/cd/Hide your heart&quot;</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:artist</span>&gt;</span>Bonnie Tyler<span class="hljs-tag">&lt;/<span class="hljs-name">cd:artist</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:country</span>&gt;</span>UK<span class="hljs-tag">&lt;/<span class="hljs-name">cd:country</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:company</span>&gt;</span>CBS Records<span class="hljs-tag">&lt;/<span class="hljs-name">cd:company</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:price</span>&gt;</span>9.90<span class="hljs-tag">&lt;/<span class="hljs-name">cd:price</span>&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">cd:year</span>&gt;</span>1988<span class="hljs-tag">&lt;/<span class="hljs-name">cd:year</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">rdf:Description</span>&gt;</span><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">rdf:RDF</span>&gt;</span> <br></code></pre></td></tr></table></figure><p>此 RDF 文档的第一行是 XML 声明。这个 XML 声明之后是 RDF文档的根元素：<em><rdf:RDF></em>。</p><p><em>xmlns:rdf</em> 命名空间，规定了带有前缀 rdf 的元素来自命名空间"http://www.w3.org/1999/02/22-rdf-syntax-ns#"。</p><p><em>xmlns:cd</em> 命名空间，规定了带有前缀 cd 的元素来自命名空间"http://www.recshop.fake/cd#"。</p><p><em><rdf:Description></em> 元素包含了对被 <em>rdf:about</em>属性标识的资源的描述。</p><p>元素：<em><cd:artist></em>、<em><cd:country></em>、</p><p><cd:company> 等是此资源的属性。</p><ul><li>陈述的主体是：http://www.recshop.fake/cd/Empire Burlesque</li><li>谓语是：cd:artist、cd:country、cd:company、cd:price、cd:year</li><li>客体是：Bob Dylan、USA、Columbia、10.90、1985</li></ul><p> </p><p> </p><h3 id="分类法rdf-s">2.4 分类法：RDF-S</h3><p>RDF 的表达能力有限，无法区分类和对象，也无法定义和描述类的关系。RDF是对具体事物的描述，缺乏抽象能力，无法对同一类别的事物进行定义和描述，需要Schema 的引入。RDFS，即“Resource Description FrameworkSchema”模式语言作为 RDF 的补充解决了 RDF 表达能力有限的困境。</p><p> </p><h4 id="rdf-s-常用词汇">2.4.1 RDF-S 常用词汇</h4><p>RDFS 中常用的词汇：</p><ul><li>rdfs:Class：用于定义类。</li><li>rdfs:domain：用于表示该属性属于哪个类别。</li><li>rdfs:range：用于描述该属性的取值类型。</li><li>rdfs:subClassOf：用于描述该类的父类。</li></ul><p>比如，我们可以定义一个运动员类，声明该类是人的子类。</p><ul><li>rdfs:subProperty：用于描述该属性的父属性。</li></ul><p>比如，我们可以定义一个名称属性，声明中文名称和全名是名称的子类。</p><p> </p><h4 id="rdf-s-实例">2.4.2 RDF-S 实例</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">rdf:RDF</span> <span class="hljs-attr">xmlns:rdf</span>=<span class="hljs-string">&quot;http://www.w3.org/1999/02/22-rdf-syntax-ns#&quot;</span></span><br><span class="hljs-tag">         <span class="hljs-attr">xmlns:rdfs</span>=<span class="hljs-string">&quot;http://www.w3.org/2000/01/rdf-schema#&quot;</span>&gt;</span><br><br>  <span class="hljs-comment">&lt;!-- 声明一个类（Class） --&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:Class</span> <span class="hljs-attr">rdf:about</span>=<span class="hljs-string">&quot;#Person&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:label</span>&gt;</span>Person<span class="hljs-tag">&lt;/<span class="hljs-name">rdfs:label</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:comment</span>&gt;</span>A class representing a person.<span class="hljs-tag">&lt;/<span class="hljs-name">rdfs:comment</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">rdfs:Class</span>&gt;</span><br><br>  <span class="hljs-comment">&lt;!-- 声明一个属性（Property） --&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">rdf:Property</span> <span class="hljs-attr">rdf:about</span>=<span class="hljs-string">&quot;#hasName&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:label</span>&gt;</span>hasName<span class="hljs-tag">&lt;/<span class="hljs-name">rdfs:label</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:comment</span>&gt;</span>Relates a person to their name.<span class="hljs-tag">&lt;/<span class="hljs-name">rdfs:comment</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:domain</span> <span class="hljs-attr">rdf:resource</span>=<span class="hljs-string">&quot;#Person&quot;</span> /&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:range</span> <span class="hljs-attr">rdf:resource</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema#string&quot;</span> /&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">rdf:Property</span>&gt;</span><br><br>  <span class="hljs-comment">&lt;!-- 创建一个实例（Instance） --&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">rdf:Description</span> <span class="hljs-attr">rdf:about</span>=<span class="hljs-string">&quot;#John&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdf:type</span> <span class="hljs-attr">rdf:resource</span>=<span class="hljs-string">&quot;#Person&quot;</span> /&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">hasName</span>&gt;</span>John Doe<span class="hljs-tag">&lt;/<span class="hljs-name">hasName</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">rdf:Description</span>&gt;</span><br><br><span class="hljs-comment">&lt;!-- 定义子类关系（Subclass） --&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:Class</span> <span class="hljs-attr">rdf:about</span>=<span class="hljs-string">&quot;#Student&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:label</span>&gt;</span>Student<span class="hljs-tag">&lt;/<span class="hljs-name">rdfs:label</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:comment</span>&gt;</span>A subclass representing a student.<span class="hljs-tag">&lt;/<span class="hljs-name">rdfs:comment</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:subClassOf</span> <span class="hljs-attr">rdf:resource</span>=<span class="hljs-string">&quot;#Person&quot;</span> /&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">rdfs:Class</span>&gt;</span><br><br>  <span class="hljs-comment">&lt;!-- 定义属性子属性（Subproperty） --&gt;</span><br>  <span class="hljs-tag">&lt;<span class="hljs-name">rdf:Property</span> <span class="hljs-attr">rdf:about</span>=<span class="hljs-string">&quot;#hasEmail&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:subPropertyOf</span> <span class="hljs-attr">rdf:resource</span>=<span class="hljs-string">&quot;#hasName&quot;</span> /&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">rdfs:comment</span>&gt;</span>Relates a person to their email address.<span class="hljs-tag">&lt;/<span class="hljs-name">rdfs:comment</span>&gt;</span><br>  <span class="hljs-tag">&lt;/<span class="hljs-name">rdf:Property</span>&gt;</span><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">rdf:RDF</span>&gt;</span><br></code></pre></td></tr></table></figure><p> </p><p> </p><h3 id="本体owl">2.5 本体：OWL</h3><p>OWL的本体在RDFS具有的类、属性、实例的基础上，加入了关系、限制和公理。</p><p>通过 RDF(S)可以表达一些简单的语义，但在更复杂的场景下，RDF(S)语义表达能力显得太弱，还缺少诸多常用的特征。包括对局部值域的属性定义，类、属性、个体的等价性，不相交类的定义，基数约束，关于属性特征的描述等。因此W3C提出了OWL语言扩展RDF(S)，作为语义网上表示本体的推荐语言。OWL可以看作是RDFS的一个扩展，添加了额外的预定义词汇。</p><p>OWL相较于RDFS，引入了布尔算子（并、或、补）、递归地构建复杂的类，还提供了表示存在值约束、任意值约束和数量值约束等能力。同时，OWL能提供描述属性具有传递性、对称性、函数性等性质。还有两个类等价或者不相交，两个属性等价或者互逆，两个实例相同或者不同，还有枚举类等等。</p><p>OWL 描述属性常用的词汇：</p><p>描述属性特征的词汇</p><ul><li>owl:TransitiveProperty.表示该属性具有传递性质。例如，我们定义“位于”是具有传递性的属性，若A位于B，B位于C，那么A肯定位于C。</li><li>owl:SymmetricProperty.表示该属性具有对称性。例如，我们定义“认识”是具有对称性的属性，若A认识B，那么B肯定认识A。</li><li>owl:FunctionalProperty. 表示该属性取值的唯一性。例如，我们定义“母亲”是具有唯一性的属性，若A的母亲是B，在其他地方我们得知A的母亲是C，那么B和C指的是同一个人。</li><li>owl:inverseOf.定义某个属性的相反关系。例如，定义“父母”的相反关系是“子女”，若A是B的父母，那么B肯定是A的子女。</li></ul><p>本体映射词汇（Ontology Mapping）</p><ul><li>owl:equivalentClass. 表示某个类和另一个类是相同的。</li><li>owl:equivalentProperty. 表示某个属性和另一个属性是相同的。</li><li>owl:sameAs. 表示两个实体是同一个实体。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
